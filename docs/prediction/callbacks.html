<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fontai.prediction.callbacks API documentation</title>
<meta name="description" content="This module contains custom Tensorflow callbacks" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fontai.prediction.callbacks</code></h1>
</header>
<section id="section-intro">
<p>This module contains custom Tensorflow callbacks</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module contains custom Tensorflow callbacks
&#34;&#34;&#34;
import os
import typing as t

import tensorflow as tf
from tensorflow.python.keras import backend
import matplotlib.pyplot as plt
import numpy as np
import mlflow

class SAAEImageSamplerCallback(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Generates randomly chosen character images from the model at the end of each epoch and pushes them to MLFLow.
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    n_labels: int,
    embedding_dim: int,
    n_imgs=16):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
        n_imgs (int, optional): Number of images to sample
    
    
    &#34;&#34;&#34;

    self.n_labels = n_labels
    self.embedding_dim = embedding_dim
    self.n_imgs = n_imgs

    self.past_epochs = None

  def on_epoch_end(self,epoch,numpy_logs):

    if self.past_epochs is None:
      # determine whether there are previously generated images form previous runs
      active_run = mlflow.active_run()
      client = mlflow.tracking.MlflowClient()
      self.past_epochs = len(client.list_artifacts(active_run.info.run_id, self.__class__.__name__))

    logical_epoch = epoch + self.past_epochs
    output_file = f&#34;{logical_epoch}.png&#34;
    imgs = self.generate_images()
    self.plot_images(imgs, output_file)
    mlflow.log_artifact(output_file, self.__class__.__name__)
    os.remove(output_file)

  def generate_images(self):
    &#34;&#34;&#34;Produces character images stacked in a tensor from a generative decoder model
    
    Returns:
        tf.Tensor
    &#34;&#34;&#34;
    # sample encoded representation
    samples = self.model.prior_sampler(shape=(self.n_imgs,self.embedding_dim)).numpy()
    # sample one hot encoded labels
    labels = []
    for k in range(self.n_imgs):
      label = np.random.randint(0,self.n_labels,1)
      onehot = np.zeros((1,self.n_labels), dtype=np.float32)
      onehot[0,label] = 1.0
      labels.append(np.concatenate([samples[k].reshape((1,self.embedding_dim)),onehot],axis=-1))
    
    fully_encoded = np.array(labels, dtype=np.float32)

    imgs = self.model.decoder.predict(fully_encoded)
    # images are along first axis (image &lt;=&gt; example)
    return imgs


  def plot_images(self, imgs: t.Union[tf.Tensor, np.ndarray], output_file: str, n_cols = 7) -&gt; None:
    &#34;&#34;&#34;Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.
    
    Args:
        imgs (t.Union[tf.Tensor, np.ndarray]): 4-dimensional array of images
        output_file (str): output file
        n_cols (int, optional): number of columns in output figure
    &#34;&#34;&#34;
    # plot multiple images
    n_imgs, height, width, c = imgs.shape
    n_rows = int(np.ceil(n_imgs/n_cols))

    fig, axs = plt.subplots(n_rows, n_cols)
    for i in range(n_imgs):
      x = imgs[i]
      if isinstance(x, np.ndarray):
        x_ = x
      else:
        x_ = x.numpy()
      np_x = (255 * x_).astype(np.uint8).reshape((height,width))
      axs[int(i/n_cols), i%n_cols].imshow(np_x, cmap=&#34;Greys&#34;)

    # get rid of subfigure axes
    for k in range(n_rows*n_cols):
      axs[int(k/n_cols), k%n_cols].axis(&#34;off&#34;)
    
    plt.savefig(output_file)

class SAAEFontSamplerCallback(SAAEImageSamplerCallback):

  &#34;&#34;&#34;Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model generates a single font&#39;s character at a time, depending on the provided input label
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    n_labels: int,
    embedding_dim: int):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
    
    &#34;&#34;&#34;

    super().__init__(n_labels,embedding_dim,n_labels)

  def generate_images(self):

    # sample encoded representation
    sample = self.model.prior_sampler(shape=(1,self.embedding_dim)).numpy()
    # sample one hot encoded labels
    labels = []
    for k in range(self.n_labels):
      onehot = np.zeros((1,self.n_labels), dtype=np.float32)
      onehot[0,k] = 1.0
      labels.append(np.concatenate([sample,onehot],axis=-1))
    
    fully_encoded = np.array(labels, dtype=np.float32)

    imgs = self.model.decoder.predict(fully_encoded)
    return imgs



class TensorSAAEFontSamplerCallback(SAAEImageSamplerCallback):

  &#34;&#34;&#34;Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model produces all character images as a 3d Tensor where each channel represent one of the font&#39;s characters.
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    embedding_dim: int):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
    
    &#34;&#34;&#34;

    super().__init__(None,embedding_dim,None)

  def generate_images(self):
    &#34;&#34;&#34;Generates images as Tensor. Images are along the last axis (images &lt;=&gt;channels)
    
    Returns:
        tf.Tensor
    &#34;&#34;&#34;
    # sample encoded representation
    samples = self.model.prior_sampler(shape=(1,self.embedding_dim)).numpy()
    imgs = self.model.decoder.predict(samples)
    return imgs


  def plot_images(self, imgs: t.Union[tf.Tensor, np.ndarray], output_file: str, n_cols = 7) -&gt; None:
    &#34;&#34;&#34;Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.
    
    Args:
        imgs (t.Union[tf.Tensor, np.ndarray]): 4-dimensional array of images
        output_file (str): output file
        n_cols (int, optional): number of columns in output figure
    &#34;&#34;&#34;
    # plot multiple images
    n_fonts, height, width, n_imgs = imgs.shape
    n_rows = int(np.ceil(n_imgs/n_cols))

    fig, axs = plt.subplots(n_rows, n_cols)
    for i in range(n_imgs):
      x = imgs[0,:,:,i]
      if isinstance(x, np.ndarray):
        x_ = x
      else:
        x_ = x.numpy()
      np_x = (255 * x_).astype(np.uint8).reshape((height,width))
      axs[int(i/n_cols), i%n_cols].imshow(np_x, cmap=&#34;Greys&#34;)

    # get rid of subfigure axes
    for k in range(n_rows*n_cols):
      axs[int(k/n_cols), k%n_cols].axis(&#34;off&#34;)
    
    plt.savefig(output_file)




class SAAELRHalver(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Halves the step size of every embedded model in a custom supervised adversarial autoencoder as defined in the `models` submodule, up to a minimum accepted step size
  
  Attributes:
      halve_after (int): number of epochs after which step sizes are halved
      min_lr (float): lower bound for step size
  &#34;&#34;&#34;

  def __init__(self, halve_after: int = 10, min_lr: float = 0.0001):

    self.halve_after = 10
    self.min_lr = min_lr
    self.initial_lr = None

  def on_epoch_begin(self, epoch, logs=None):

    # assumes the LR for all models is the same
    for model in self.model.model_list:
      model_lr = getattr(self.model, model).optimizer.lr
      if self.initial_lr is None:
        self.initial_lr = float(backend.get_value(model_lr))
      lr = max(self.initial_lr/2**int(epoch/self.halve_after), self.min_lr)
      backend.set_value(model_lr, backend.get_value(lr))


class SAAESnapshot(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Saves the model every certain number of iterations
  
  Attributes:
      frequency (int): frequency of snapshots in epochs
      snapshot_path (str): output path
  
  &#34;&#34;&#34;

  def __init__(self, snapshot_path: str, frequency: int = 10):

    self.frequency = frequency
    self.snapshot_path = snapshot_path

  def on_epoch_end(self, epoch, logs=None):

    if epoch &gt; 1 and epoch % self.frequency == 0:
      self.model.save(self.snapshot_path)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fontai.prediction.callbacks.SAAEFontSamplerCallback"><code class="flex name class">
<span>class <span class="ident">SAAEFontSamplerCallback</span></span>
<span>(</span><span>n_labels: int, embedding_dim: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model generates a single font's character at a time, depending on the provided input label</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_labels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of labels in model's charset</dd>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of encoded representation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SAAEFontSamplerCallback(SAAEImageSamplerCallback):

  &#34;&#34;&#34;Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model generates a single font&#39;s character at a time, depending on the provided input label
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    n_labels: int,
    embedding_dim: int):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
    
    &#34;&#34;&#34;

    super().__init__(n_labels,embedding_dim,n_labels)

  def generate_images(self):

    # sample encoded representation
    sample = self.model.prior_sampler(shape=(1,self.embedding_dim)).numpy()
    # sample one hot encoded labels
    labels = []
    for k in range(self.n_labels):
      onehot = np.zeros((1,self.n_labels), dtype=np.float32)
      onehot[0,k] = 1.0
      labels.append(np.concatenate([sample,onehot],axis=-1))
    
    fully_encoded = np.array(labels, dtype=np.float32)

    imgs = self.model.decoder.predict(fully_encoded)
    return imgs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback">SAAEImageSamplerCallback</a></li>
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback">SAAEImageSamplerCallback</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.generate_images" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.generate_images">generate_images</a></code></li>
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end">on_epoch_end</a></code></li>
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images">plot_images</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fontai.prediction.callbacks.SAAEImageSamplerCallback"><code class="flex name class">
<span>class <span class="ident">SAAEImageSamplerCallback</span></span>
<span>(</span><span>n_labels: int, embedding_dim: int, n_imgs=16)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates randomly chosen character images from the model at the end of each epoch and pushes them to MLFLow.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_labels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of labels in model's charset</dd>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of encoded representation</dd>
<dt><strong><code>n_imgs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of images to sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SAAEImageSamplerCallback(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Generates randomly chosen character images from the model at the end of each epoch and pushes them to MLFLow.
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    n_labels: int,
    embedding_dim: int,
    n_imgs=16):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
        n_imgs (int, optional): Number of images to sample
    
    
    &#34;&#34;&#34;

    self.n_labels = n_labels
    self.embedding_dim = embedding_dim
    self.n_imgs = n_imgs

    self.past_epochs = None

  def on_epoch_end(self,epoch,numpy_logs):

    if self.past_epochs is None:
      # determine whether there are previously generated images form previous runs
      active_run = mlflow.active_run()
      client = mlflow.tracking.MlflowClient()
      self.past_epochs = len(client.list_artifacts(active_run.info.run_id, self.__class__.__name__))

    logical_epoch = epoch + self.past_epochs
    output_file = f&#34;{logical_epoch}.png&#34;
    imgs = self.generate_images()
    self.plot_images(imgs, output_file)
    mlflow.log_artifact(output_file, self.__class__.__name__)
    os.remove(output_file)

  def generate_images(self):
    &#34;&#34;&#34;Produces character images stacked in a tensor from a generative decoder model
    
    Returns:
        tf.Tensor
    &#34;&#34;&#34;
    # sample encoded representation
    samples = self.model.prior_sampler(shape=(self.n_imgs,self.embedding_dim)).numpy()
    # sample one hot encoded labels
    labels = []
    for k in range(self.n_imgs):
      label = np.random.randint(0,self.n_labels,1)
      onehot = np.zeros((1,self.n_labels), dtype=np.float32)
      onehot[0,label] = 1.0
      labels.append(np.concatenate([samples[k].reshape((1,self.embedding_dim)),onehot],axis=-1))
    
    fully_encoded = np.array(labels, dtype=np.float32)

    imgs = self.model.decoder.predict(fully_encoded)
    # images are along first axis (image &lt;=&gt; example)
    return imgs


  def plot_images(self, imgs: t.Union[tf.Tensor, np.ndarray], output_file: str, n_cols = 7) -&gt; None:
    &#34;&#34;&#34;Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.
    
    Args:
        imgs (t.Union[tf.Tensor, np.ndarray]): 4-dimensional array of images
        output_file (str): output file
        n_cols (int, optional): number of columns in output figure
    &#34;&#34;&#34;
    # plot multiple images
    n_imgs, height, width, c = imgs.shape
    n_rows = int(np.ceil(n_imgs/n_cols))

    fig, axs = plt.subplots(n_rows, n_cols)
    for i in range(n_imgs):
      x = imgs[i]
      if isinstance(x, np.ndarray):
        x_ = x
      else:
        x_ = x.numpy()
      np_x = (255 * x_).astype(np.uint8).reshape((height,width))
      axs[int(i/n_cols), i%n_cols].imshow(np_x, cmap=&#34;Greys&#34;)

    # get rid of subfigure axes
    for k in range(n_rows*n_cols):
      axs[int(k/n_cols), k%n_cols].axis(&#34;off&#34;)
    
    plt.savefig(output_file)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="fontai.prediction.callbacks.SAAEFontSamplerCallback" href="#fontai.prediction.callbacks.SAAEFontSamplerCallback">SAAEFontSamplerCallback</a></li>
<li><a title="fontai.prediction.callbacks.TensorSAAEFontSamplerCallback" href="#fontai.prediction.callbacks.TensorSAAEFontSamplerCallback">TensorSAAEFontSamplerCallback</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.callbacks.SAAEImageSamplerCallback.generate_images"><code class="name flex">
<span>def <span class="ident">generate_images</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Produces character images stacked in a tensor from a generative decoder model</p>
<h2 id="returns">Returns</h2>
<p>tf.Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_images(self):
  &#34;&#34;&#34;Produces character images stacked in a tensor from a generative decoder model
  
  Returns:
      tf.Tensor
  &#34;&#34;&#34;
  # sample encoded representation
  samples = self.model.prior_sampler(shape=(self.n_imgs,self.embedding_dim)).numpy()
  # sample one hot encoded labels
  labels = []
  for k in range(self.n_imgs):
    label = np.random.randint(0,self.n_labels,1)
    onehot = np.zeros((1,self.n_labels), dtype=np.float32)
    onehot[0,label] = 1.0
    labels.append(np.concatenate([samples[k].reshape((1,self.embedding_dim)),onehot],axis=-1))
  
  fully_encoded = np.array(labels, dtype=np.float32)

  imgs = self.model.decoder.predict(fully_encoded)
  # images are along first axis (image &lt;=&gt; example)
  return imgs</code></pre>
</details>
</dd>
<dt id="fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end"><code class="name flex">
<span>def <span class="ident">on_epoch_end</span></span>(<span>self, epoch, numpy_logs)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<h2 id="arguments">Arguments</h2>
<p>epoch: integer, index of epoch.
logs: dict, metric results for this training epoch, and for the
validation epoch if validation is performed. Validation result keys
are prefixed with <code>val_</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_epoch_end(self,epoch,numpy_logs):

  if self.past_epochs is None:
    # determine whether there are previously generated images form previous runs
    active_run = mlflow.active_run()
    client = mlflow.tracking.MlflowClient()
    self.past_epochs = len(client.list_artifacts(active_run.info.run_id, self.__class__.__name__))

  logical_epoch = epoch + self.past_epochs
  output_file = f&#34;{logical_epoch}.png&#34;
  imgs = self.generate_images()
  self.plot_images(imgs, output_file)
  mlflow.log_artifact(output_file, self.__class__.__name__)
  os.remove(output_file)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images"><code class="name flex">
<span>def <span class="ident">plot_images</span></span>(<span>self, imgs: Union[tensorflow.python.framework.ops.Tensor, numpy.ndarray], output_file: str, n_cols=7) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>imgs</code></strong> :&ensp;<code>t.Union[tf.Tensor, np.ndarray]</code></dt>
<dd>4-dimensional array of images</dd>
<dt><strong><code>output_file</code></strong> :&ensp;<code>str</code></dt>
<dd>output file</dd>
<dt><strong><code>n_cols</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of columns in output figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_images(self, imgs: t.Union[tf.Tensor, np.ndarray], output_file: str, n_cols = 7) -&gt; None:
  &#34;&#34;&#34;Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.
  
  Args:
      imgs (t.Union[tf.Tensor, np.ndarray]): 4-dimensional array of images
      output_file (str): output file
      n_cols (int, optional): number of columns in output figure
  &#34;&#34;&#34;
  # plot multiple images
  n_imgs, height, width, c = imgs.shape
  n_rows = int(np.ceil(n_imgs/n_cols))

  fig, axs = plt.subplots(n_rows, n_cols)
  for i in range(n_imgs):
    x = imgs[i]
    if isinstance(x, np.ndarray):
      x_ = x
    else:
      x_ = x.numpy()
    np_x = (255 * x_).astype(np.uint8).reshape((height,width))
    axs[int(i/n_cols), i%n_cols].imshow(np_x, cmap=&#34;Greys&#34;)

  # get rid of subfigure axes
  for k in range(n_rows*n_cols):
    axs[int(k/n_cols), k%n_cols].axis(&#34;off&#34;)
  
  plt.savefig(output_file)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.callbacks.SAAELRHalver"><code class="flex name class">
<span>class <span class="ident">SAAELRHalver</span></span>
<span>(</span><span>halve_after: int = 10, min_lr: float = 0.0001)</span>
</code></dt>
<dd>
<div class="desc"><p>Halves the step size of every embedded model in a custom supervised adversarial autoencoder as defined in the <code>models</code> submodule, up to a minimum accepted step size</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>halve_after</code></strong> :&ensp;<code>int</code></dt>
<dd>number of epochs after which step sizes are halved</dd>
<dt><strong><code>min_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>lower bound for step size</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SAAELRHalver(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Halves the step size of every embedded model in a custom supervised adversarial autoencoder as defined in the `models` submodule, up to a minimum accepted step size
  
  Attributes:
      halve_after (int): number of epochs after which step sizes are halved
      min_lr (float): lower bound for step size
  &#34;&#34;&#34;

  def __init__(self, halve_after: int = 10, min_lr: float = 0.0001):

    self.halve_after = 10
    self.min_lr = min_lr
    self.initial_lr = None

  def on_epoch_begin(self, epoch, logs=None):

    # assumes the LR for all models is the same
    for model in self.model.model_list:
      model_lr = getattr(self.model, model).optimizer.lr
      if self.initial_lr is None:
        self.initial_lr = float(backend.get_value(model_lr))
      lr = max(self.initial_lr/2**int(epoch/self.halve_after), self.min_lr)
      backend.set_value(model_lr, backend.get_value(lr))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.callbacks.SAAELRHalver.on_epoch_begin"><code class="name flex">
<span>def <span class="ident">on_epoch_begin</span></span>(<span>self, epoch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the start of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<h2 id="arguments">Arguments</h2>
<p>epoch: integer, index of epoch.
logs: dict. Currently no data is passed to this argument for this method
but that may change in the future.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_epoch_begin(self, epoch, logs=None):

  # assumes the LR for all models is the same
  for model in self.model.model_list:
    model_lr = getattr(self.model, model).optimizer.lr
    if self.initial_lr is None:
      self.initial_lr = float(backend.get_value(model_lr))
    lr = max(self.initial_lr/2**int(epoch/self.halve_after), self.min_lr)
    backend.set_value(model_lr, backend.get_value(lr))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.callbacks.SAAESnapshot"><code class="flex name class">
<span>class <span class="ident">SAAESnapshot</span></span>
<span>(</span><span>snapshot_path: str, frequency: int = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the model every certain number of iterations</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>frequency</code></strong> :&ensp;<code>int</code></dt>
<dd>frequency of snapshots in epochs</dd>
<dt><strong><code>snapshot_path</code></strong> :&ensp;<code>str</code></dt>
<dd>output path</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SAAESnapshot(tf.keras.callbacks.Callback):

  &#34;&#34;&#34;Saves the model every certain number of iterations
  
  Attributes:
      frequency (int): frequency of snapshots in epochs
      snapshot_path (str): output path
  
  &#34;&#34;&#34;

  def __init__(self, snapshot_path: str, frequency: int = 10):

    self.frequency = frequency
    self.snapshot_path = snapshot_path

  def on_epoch_end(self, epoch, logs=None):

    if epoch &gt; 1 and epoch % self.frequency == 0:
      self.model.save(self.snapshot_path)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.callbacks.SAAESnapshot.on_epoch_end"><code class="name flex">
<span>def <span class="ident">on_epoch_end</span></span>(<span>self, epoch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<h2 id="arguments">Arguments</h2>
<p>epoch: integer, index of epoch.
logs: dict, metric results for this training epoch, and for the
validation epoch if validation is performed. Validation result keys
are prefixed with <code>val_</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_epoch_end(self, epoch, logs=None):

  if epoch &gt; 1 and epoch % self.frequency == 0:
    self.model.save(self.snapshot_path)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.callbacks.TensorSAAEFontSamplerCallback"><code class="flex name class">
<span>class <span class="ident">TensorSAAEFontSamplerCallback</span></span>
<span>(</span><span>embedding_dim: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model produces all character images as a 3d Tensor where each channel represent one of the font's characters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_labels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of labels in model's charset</dd>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of encoded representation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorSAAEFontSamplerCallback(SAAEImageSamplerCallback):

  &#34;&#34;&#34;Generates a random font style from the model, generates all of its characters and pushes them to MLFLow at the end of each epoch. This callback assumes the model produces all character images as a 3d Tensor where each channel represent one of the font&#39;s characters.
  
  &#34;&#34;&#34;
  
  def __init__(
    self,
    embedding_dim: int):
    &#34;&#34;&#34;
    Args:
        n_labels (int): Number of labels in model&#39;s charset
        embedding_dim (int): Dimensionality of encoded representation
    
    &#34;&#34;&#34;

    super().__init__(None,embedding_dim,None)

  def generate_images(self):
    &#34;&#34;&#34;Generates images as Tensor. Images are along the last axis (images &lt;=&gt;channels)
    
    Returns:
        tf.Tensor
    &#34;&#34;&#34;
    # sample encoded representation
    samples = self.model.prior_sampler(shape=(1,self.embedding_dim)).numpy()
    imgs = self.model.decoder.predict(samples)
    return imgs


  def plot_images(self, imgs: t.Union[tf.Tensor, np.ndarray], output_file: str, n_cols = 7) -&gt; None:
    &#34;&#34;&#34;Utility function to plot a sequence of characters and save it in a given location as a single tiled figure.
    
    Args:
        imgs (t.Union[tf.Tensor, np.ndarray]): 4-dimensional array of images
        output_file (str): output file
        n_cols (int, optional): number of columns in output figure
    &#34;&#34;&#34;
    # plot multiple images
    n_fonts, height, width, n_imgs = imgs.shape
    n_rows = int(np.ceil(n_imgs/n_cols))

    fig, axs = plt.subplots(n_rows, n_cols)
    for i in range(n_imgs):
      x = imgs[0,:,:,i]
      if isinstance(x, np.ndarray):
        x_ = x
      else:
        x_ = x.numpy()
      np_x = (255 * x_).astype(np.uint8).reshape((height,width))
      axs[int(i/n_cols), i%n_cols].imshow(np_x, cmap=&#34;Greys&#34;)

    # get rid of subfigure axes
    for k in range(n_rows*n_cols):
      axs[int(k/n_cols), k%n_cols].axis(&#34;off&#34;)
    
    plt.savefig(output_file)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback">SAAEImageSamplerCallback</a></li>
<li>tensorflow.python.keras.callbacks.Callback</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.callbacks.TensorSAAEFontSamplerCallback.generate_images"><code class="name flex">
<span>def <span class="ident">generate_images</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates images as Tensor. Images are along the last axis (images &lt;=&gt;channels)</p>
<h2 id="returns">Returns</h2>
<p>tf.Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_images(self):
  &#34;&#34;&#34;Generates images as Tensor. Images are along the last axis (images &lt;=&gt;channels)
  
  Returns:
      tf.Tensor
  &#34;&#34;&#34;
  # sample encoded representation
  samples = self.model.prior_sampler(shape=(1,self.embedding_dim)).numpy()
  imgs = self.model.decoder.predict(samples)
  return imgs</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback">SAAEImageSamplerCallback</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end">on_epoch_end</a></code></li>
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images">plot_images</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fontai.prediction" href="index.html">fontai.prediction</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fontai.prediction.callbacks.SAAEFontSamplerCallback" href="#fontai.prediction.callbacks.SAAEFontSamplerCallback">SAAEFontSamplerCallback</a></code></h4>
</li>
<li>
<h4><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback">SAAEImageSamplerCallback</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.generate_images" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.generate_images">generate_images</a></code></li>
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.on_epoch_end">on_epoch_end</a></code></li>
<li><code><a title="fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images" href="#fontai.prediction.callbacks.SAAEImageSamplerCallback.plot_images">plot_images</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.callbacks.SAAELRHalver" href="#fontai.prediction.callbacks.SAAELRHalver">SAAELRHalver</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.callbacks.SAAELRHalver.on_epoch_begin" href="#fontai.prediction.callbacks.SAAELRHalver.on_epoch_begin">on_epoch_begin</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.callbacks.SAAESnapshot" href="#fontai.prediction.callbacks.SAAESnapshot">SAAESnapshot</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.callbacks.SAAESnapshot.on_epoch_end" href="#fontai.prediction.callbacks.SAAESnapshot.on_epoch_end">on_epoch_end</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.callbacks.TensorSAAEFontSamplerCallback" href="#fontai.prediction.callbacks.TensorSAAEFontSamplerCallback">TensorSAAEFontSamplerCallback</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.callbacks.TensorSAAEFontSamplerCallback.generate_images" href="#fontai.prediction.callbacks.TensorSAAEFontSamplerCallback.generate_images">generate_images</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>