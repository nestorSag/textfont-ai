<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fontai.prediction.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fontai.prediction.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import sys
import re
import io
import tensorflow as tf
import json
import logging
import copy
import matplotlib.pyplot as plt
import numpy as np

from fontai.io.storage import BytestreamPath

logger = logging.getLogger(__name__)

class CharStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This class fits a supervised adversarial autoencoder and its inspired in the architecture from &#34;Adversarial Autoencoders&#34; by Ian Goodfellow et al. The only difference is that label (i.e. character) information is injected between the encoder and the style embedding, in the hope that labels not only help the decoding but also the encoding process, e.g. curvyness shouldn&#39;t be as important in the input if its a C than if its an H.
  
  Attributes:
      accuracy_metric (tf.keras.metrics.Accuracy): Accuracy metric
      cross_entropy (tf.keras.losses.BinaryCrossentropy): Cross entropy loss
      decoder (tf.keras.Model): Decoder model that maps style and characters to images
      prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
      full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
      image_encoder (tf.keras.Model): Encoder for image features
      input_dim (t.Tuple[int]): Input dimension
      mse_loss (TYPE): Description
      mse_metric (tf.keras.losses.MSE): MSE loss
      prior_batch_size (int): Batch size from prior distribution at training time
      rec_loss_weight (float): Weight of reconstruction loss at training time. Should be between 0 and 1.
  &#34;&#34;&#34;

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs probabilities(i.e. in [0, 1])

  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(CharStyleSAAE, self).__init__()

    #encoder.build(input_shape=input_dim)
    #decoder.build(input_shape=(None,n_classes+code_dim))
    #prior_discriminator.build(input_shape=(None,code_dim))

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size

    self.prior_sampler = tf.random.normal
    
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.decoder.compile(optimizer = copy.deepcopy(optimizer))
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def train_step(self, inputs):
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(x, training=True)
      full_precode = tf.concat([image_precode, labels], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,labels],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      #print((self.prior_batch_size,code.shape[1]))
      prior_samples = self.prior_sampler(shape=[self.prior_batch_size,code.shape[1]])
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(x,decoded)
      classification_loss = self.prior_discriminator_loss(real,fake)
      mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)


    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    #self.cross_entropy_metric.update_state(discr_true, discr_predicted)

    return {&#34;reconstruction MSE&#34;: self.mse_metric.result(), &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}

  @property
  def metrics(self):
    &#34;&#34;&#34;Performance metrics to report at training time
    
    Returns: A list of metric objects

    &#34;&#34;&#34;
    return [self.mse_metric, self.prior_accuracy_metric, self.cross_entropy_metric]

  def predict(self, *args, **kwargs):
    return self.image_encoder.predict(*args,**kwargs)


  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;
    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size
    }

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)


















class PureCharStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model is trained as a regular SAAE but an additional discriminator model is added to ensure the embedding does not retain information about the character class; i.e. it only retains style information
  &#34;&#34;&#34;
  mean_metric = tf.keras.metrics.Mean(name=&#34;Mean code variance&#34;)
  char_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;style-char accuracy&#34;)
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)


  style_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)
  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    char_discriminator: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        char_discriminator (tf.keras.Model): Discriminator to remove any character information from embeddings
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(PureCharStyleSAAE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size
    self.char_discriminator = char_discriminator

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;, &#34;char_discriminator&#34;]

  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.decoder.compile(optimizer = copy.deepcopy(optimizer))
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))
    self.char_discriminator.compile(optimizer=copy.deepcopy(optimizer))

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(x, training=True)
      full_precode = tf.concat([image_precode, labels], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,labels],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(self.prior_batch_size,code.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)

      # apply char_discriminator model
      char_guess = self.char_discriminator(code,training=True)

      # compute losses for the models
      char_loss = self.style_loss(labels, char_guess)/self.prior_batch_size
      reconstruction_loss = tf.keras.losses.MSE(x,decoded)
      classification_loss = self.prior_discriminator_loss(real,fake)

      mixed_loss = -(1-self.rec_loss_weight)*(classification_loss + char_loss) + self.rec_loss_weight*(reconstruction_loss)

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
    style_discr_gradients = tape.gradient(char_loss, self.char_discriminator.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
    self.char_discriminator.optimizer.apply_gradients(
      zip(style_discr_gradients, self.char_discriminator.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    #self.cross_entropy_metric.update_state(discr_true, discr_predicted)


    self.char_accuracy_metric.update_state(tf.argmax(labels, axis=-1), tf.argmax(char_guess, axis=-1))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result(), 
    &#34;sstyle discriminator accuracy&#34;: self.char_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.char_discriminator.save(str(BytestreamPath(output_dir) / &#34;char_discriminator&#34;))
    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size
    }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)
    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    char_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;char_discriminator&#34;))
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())

    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      char_discriminator = char_discriminator,
      **d)







class PureFontStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model is trained on all characters from one or more font files at a time; the aim is to encode the font&#39;s style as opposed to single characters&#39; styles, which can happen when training with scrambled characters from different fonts and results in sometimes having different-looking image styles for a given style in latent space. This model works with characters from a single typeface at a time, and use the style from a given character to decode a different randomly chosen character in the same font, using the encoded style and target label information. 
  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32,
    code_regularisation_weight=0):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(PureFontStyleSAAE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size

    self.prior_sampler = tf.random.normal
    self.code_regularisation_weight = code_regularisation_weight
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]


  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
    &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
    
    Args:
        x (tf.Tensor): Feature tensor
        labels (tf.Tensor): Label tensor
    
    Returns:
        t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
    &#34;&#34;&#34;
    #
    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    #
    style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

    outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    #
    for k in range(n_fonts):
      x_k, labels_k = x[k], labels[k]
      x_k_shape = tf.shape(x_k)
      shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
      scrambled = tf.random.shuffle(shuffling_idx)
      #
      style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
      style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

      outcome_x = outcome_x.write(k, x_k)
      outcome_y = outcome_y.write(k, labels_k)
      #
    #
    #
    return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

    n_examples = tf.shape(style_x)[0]
    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(style_x, training=True)
      full_precode = tf.concat([image_precode, style_y], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,outcome_y],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)


      # Moment regularization for embedded representation to keep it closer to standard normal
      reg = self.code_regularisation_weight*(tf.reduce_mean(code)**2 + (tf.reduce_mean(code**2) - 1.0)**2)/2

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
      classification_loss = self.prior_discriminator_loss(real,fake)

      mixed_loss = -(1-self.rec_loss_weight)*(classification_loss) + self.rec_loss_weight*(reconstruction_loss + reg)


    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(outcome_x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size,
      &#34;code_regularisation_weight&#34;: self.code_regularisation_weight
    }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())
    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)
















class PureFontStyleSA2AE(tf.keras.Model):

  &#34;&#34;&#34;This model is works like PureFontStyleSA2AE but instead of minimising the MSE reconstruction error, an additional image discriminator classify between real and reconstructed images, so the encoder and decoder now maximise misclassification error.

  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;adversarial prior accuracy&#34;)
  image_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;adversarial image accuracy&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    image_discriminator: tf.keras.Model,
    code_regularisation_weight=0):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        image_discriminator (tf.keras.Model): image discriminator
    &#34;&#34;&#34;
    super(PureFontStyleSA2AE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.image_discriminator = image_discriminator

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;, &#34;image_discriminator&#34;]


  def discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
    &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
    
    Args:
        x (tf.Tensor): Feature tensor
        labels (tf.Tensor): Label tensor
    
    Returns:
        t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
    &#34;&#34;&#34;
    #
    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    #
    style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

    outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    #
    for k in range(n_fonts):
      x_k, labels_k = x[k], labels[k]
      x_k_shape = tf.shape(x_k)
      shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
      scrambled = tf.random.shuffle(shuffling_idx)
      #
      style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
      style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

      outcome_x = outcome_x.write(k, x_k)
      outcome_y = outcome_y.write(k, labels_k)
      #
    #
    #
    return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

    n_examples = tf.shape(style_x)[0]
    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(style_x, training=True)
      full_precode = tf.concat([image_precode, style_y], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,outcome_y],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
      real_prior = self.prior_discriminator(prior_samples,training=True)
      fake_prior = self.prior_discriminator(code,training=True)
      prior_classification_loss = self.discriminator_loss(real_prior,fake_prior)

      #apply image_discriminator model
      real_image = self.image_discriminator(outcome_x)
      fake_image = self.image_discriminator(decoded)
      image_classification_loss = self.discriminator_loss(real_image, fake_image)

      # compute losses for the models
      #reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
      
      mixed_loss = -(prior_classification_loss + image_classification_loss)

      #mixed_loss = -(1-self.rec_loss_weight)*(prior_classification_loss) + self.rec_loss_weight*(reconstruction_loss)


    # Compute gradients
    prior_discriminator_gradients = tape.gradient(prior_classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(image_classification_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
    image_discriminator_gradients = tape.gradient(image_classification_loss, self.image_discriminator.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(prior_discriminator_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
    self.image_discriminator.optimizer.apply_gradients(zip(image_discriminator_gradients, self.image_discriminator.trainable_variables))

    # compute metrics
    #self.mse_metric.update_state(outcome_x,decoded)

    discr_true = tf.concat([tf.ones_like(real_prior),tf.zeros_like(fake_prior)],axis=0)
    discr_predicted = tf.concat([real_prior,fake_prior],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    discr_true = tf.concat([tf.ones_like(real_image),tf.zeros_like(fake_image)],axis=0)
    discr_predicted = tf.concat([real_image,fake_image],axis=0)
    self.image_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;image discriminator accuracy&#34;: self.image_accuracy_metric.result(), 
    &#34;prior discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))
    self.image_discriminator.save(str(BytestreamPath(output_dir) / &#34;image_discriminator&#34;))


    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))
    image_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_discriminator&#34;))

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      image_discriminator = image_discriminator)







class TensorFontStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model treats characters in a font as channels in an image. The encoder takes the whole font as an image with (n_char) channels.
  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(TensorFontStyleSAAE, self).__init__()

    self.encoder = encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]


  def prior_discriminator_loss(self,real,fake, n_fonts):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/tf.cast(n_fonts, tf.float32)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.encoder(x, training=training)

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, _ = inputs

    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    n_chars = x_shape[1]
    height = x_shape[2]
    width = x_shape[3]

    x = tf.reshape(x, (n_fonts, n_chars, height, width))
    x = tf.transpose(x, [0,2,3,1])

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      embedding = self.encoder(x, training=True)
      decoded = self.decoder(embedding,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_fonts,embedding.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(embedding,training=True)

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(x,decoded) 
      classification_loss = self.prior_discriminator_loss(real,fake, n_fonts)

      mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    encoder_gradients = tape.gradient(mixed_loss, self.encoder.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.encoder.optimizer.apply_gradients(zip(encoder_gradients,self.encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.encoder.save(str(BytestreamPath(output_dir) / &#34;encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight
      }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())
    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      encoder = encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fontai.prediction.models.CharStyleSAAE"><code class="flex name class">
<span>class <span class="ident">CharStyleSAAE</span></span>
<span>(</span><span>full_encoder: tensorflow.python.keras.engine.training.Model, image_encoder: tensorflow.python.keras.engine.training.Model, decoder: tensorflow.python.keras.engine.training.Model, prior_discriminator: tensorflow.python.keras.engine.training.Model, reconstruction_loss_weight: float = 0.5, prior_batch_size: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>This class fits a supervised adversarial autoencoder and its inspired in the architecture from "Adversarial Autoencoders" by Ian Goodfellow et al. The only difference is that label (i.e. character) information is injected between the encoder and the style embedding, in the hope that labels not only help the decoding but also the encoding process, e.g. curvyness shouldn't be as important in the input if its a C than if its an H.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>accuracy_metric</code></strong> :&ensp;<code>tf.keras.metrics.Accuracy</code></dt>
<dd>Accuracy metric</dd>
<dt><strong><code>cross_entropy</code></strong> :&ensp;<code>tf.keras.losses.BinaryCrossentropy</code></dt>
<dd>Cross entropy loss</dd>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>image_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>input_dim</code></strong> :&ensp;<code>t.Tuple[int]</code></dt>
<dd>Input dimension</dd>
<dt><strong><code>mse_loss</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>Description</dd>
<dt><strong><code>mse_metric</code></strong> :&ensp;<code>tf.keras.losses.MSE</code></dt>
<dd>MSE loss</dd>
<dt><strong><code>prior_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size from prior distribution at training time</dd>
<dt><strong><code>rec_loss_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight of reconstruction loss at training time. Should be between 0 and 1.</dd>
</dl>
<p>Summary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>image_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>reconstruction_loss_weight</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Weight of reconstruction loss at training time. Should be between 0 and 1.</dd>
<dt><strong><code>n_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>number of labeled classes</dd>
<dt><strong><code>prior_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size from prior distribution at training time</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CharStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This class fits a supervised adversarial autoencoder and its inspired in the architecture from &#34;Adversarial Autoencoders&#34; by Ian Goodfellow et al. The only difference is that label (i.e. character) information is injected between the encoder and the style embedding, in the hope that labels not only help the decoding but also the encoding process, e.g. curvyness shouldn&#39;t be as important in the input if its a C than if its an H.
  
  Attributes:
      accuracy_metric (tf.keras.metrics.Accuracy): Accuracy metric
      cross_entropy (tf.keras.losses.BinaryCrossentropy): Cross entropy loss
      decoder (tf.keras.Model): Decoder model that maps style and characters to images
      prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
      full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
      image_encoder (tf.keras.Model): Encoder for image features
      input_dim (t.Tuple[int]): Input dimension
      mse_loss (TYPE): Description
      mse_metric (tf.keras.losses.MSE): MSE loss
      prior_batch_size (int): Batch size from prior distribution at training time
      rec_loss_weight (float): Weight of reconstruction loss at training time. Should be between 0 and 1.
  &#34;&#34;&#34;

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs probabilities(i.e. in [0, 1])

  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(CharStyleSAAE, self).__init__()

    #encoder.build(input_shape=input_dim)
    #decoder.build(input_shape=(None,n_classes+code_dim))
    #prior_discriminator.build(input_shape=(None,code_dim))

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size

    self.prior_sampler = tf.random.normal
    
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.decoder.compile(optimizer = copy.deepcopy(optimizer))
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def train_step(self, inputs):
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(x, training=True)
      full_precode = tf.concat([image_precode, labels], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,labels],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      #print((self.prior_batch_size,code.shape[1]))
      prior_samples = self.prior_sampler(shape=[self.prior_batch_size,code.shape[1]])
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(x,decoded)
      classification_loss = self.prior_discriminator_loss(real,fake)
      mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)


    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    #self.cross_entropy_metric.update_state(discr_true, discr_predicted)

    return {&#34;reconstruction MSE&#34;: self.mse_metric.result(), &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}

  @property
  def metrics(self):
    &#34;&#34;&#34;Performance metrics to report at training time
    
    Returns: A list of metric objects

    &#34;&#34;&#34;
    return [self.mse_metric, self.prior_accuracy_metric, self.cross_entropy_metric]

  def predict(self, *args, **kwargs):
    return self.image_encoder.predict(*args,**kwargs)


  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;
    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size
    }

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.network.Network</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.prediction.models.CharStyleSAAE.cross_entropy"><code class="name">var <span class="ident">cross_entropy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.cross_entropy_metric"><code class="name">var <span class="ident">cross_entropy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.mse_metric"><code class="name">var <span class="ident">mse_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.prior_accuracy_metric"><code class="name">var <span class="ident">prior_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.prediction.models.CharStyleSAAE.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>input_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a saved instance of this class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target input folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SAAE</code></dt>
<dd>Loaded model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, input_dir: str):
  &#34;&#34;&#34;Loads a saved instance of this class
  
  Args:
      input_dir (str): Target input folder
  
  Returns:
      SAAE: Loaded model
  &#34;&#34;&#34;
  full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
  image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
  decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
  prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

  d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
  d = json.loads(d_string)

  # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
  #   d = json.loads(f.read())

  return cls(
    image_encoder = image_encoder, 
    full_encoder = full_encoder, 
    decoder = decoder, 
    prior_discriminator = prior_discriminator, 
    **d)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="fontai.prediction.models.CharStyleSAAE.metrics"><code class="name">var <span class="ident">metrics</span></code></dt>
<dd>
<div class="desc"><p>Performance metrics to report at training time</p>
<p>Returns: A list of metric objects</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def metrics(self):
  &#34;&#34;&#34;Performance metrics to report at training time
  
  Returns: A list of metric objects

  &#34;&#34;&#34;
  return [self.mse_metric, self.prior_accuracy_metric, self.cross_entropy_metric]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.models.CharStyleSAAE.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the model for training.</p>
<h2 id="arguments">Arguments</h2>
<p>optimizer: String (name of optimizer) or optimizer instance.
See <code>tf.keras.optimizers</code>.
loss: String (name of objective function), objective function or
<code>tf.keras.losses.Loss</code> instance. See <code>tf.keras.losses</code>.
An objective function is any callable with the signature
<code>loss = fn(y_true, y_pred)</code>, where
y_true = ground truth values with shape = <code>[batch_size, d0, .. dN]</code>,
except sparse loss functions such as sparse categorical crossentropy
where shape = <code>[batch_size, d0, .. dN-1]</code>.
y_pred = predicted values with shape = <code>[batch_size, d0, .. dN]</code>.
It returns a weighted loss float tensor.
If a custom <code>Loss</code> instance is used and reduction is set to NONE,
return value has the shape [batch_size, d0, .. dN-1] ie. per-sample
or per-timestep loss values; otherwise, it is a scalar.
If the model has multiple outputs, you can use a different loss on
each output by passing a dictionary or a list of losses. The loss
value that will be minimized by the model will then be the sum of
all individual losses.
metrics: List of metrics to be evaluated by the model during training
and testing.
Each of this can be a string (name of a built-in function), function
or a <code>tf.keras.metrics.Metric</code> instance. See <code>tf.keras.metrics</code>.
Typically you will use <code>metrics=['accuracy']</code>. A function is any
callable with the signature <code>result = fn(y_true, y_pred)</code>.
To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<code>metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}</code>.
You can also pass a list (len = len(outputs)) of lists of metrics
such as <code>metrics=[['accuracy'], ['accuracy', 'mse']]</code> or
<code>metrics=['accuracy', ['accuracy', 'mse']]</code>.
When you pass the strings 'accuracy' or 'acc', we convert this to
one of <code>tf.keras.metrics.BinaryAccuracy</code>,
<code>tf.keras.metrics.CategoricalAccuracy</code>,
<code>tf.keras.metrics.SparseCategoricalAccuracy</code> based on the loss
function used and the model output shape. We do a similar conversion
for the strings 'crossentropy' and 'ce' as well.
loss_weights: Optional list or dictionary specifying scalar
coefficients (Python floats) to weight the loss contributions
of different model outputs.
The loss value that will be minimized by the model
will then be the <em>weighted sum</em> of all individual losses,
weighted by the <code>loss_weights</code> coefficients.
If a list, it is expected to have a 1:1 mapping
to the model's outputs. If a dict, it is expected to map
output names (strings) to scalar coefficients.
sample_weight_mode: If you need to do timestep-wise
sample weighting (2D weights), set this to <code>"temporal"</code>.
<code>None</code> defaults to sample-wise weights (1D).
If the model has multiple outputs, you can use a different
<code>sample_weight_mode</code> on each output by passing a
dictionary or a list of modes.
weighted_metrics: List of metrics to be evaluated and weighted
by sample_weight or class_weight during training and testing.
**kwargs: Any additional arguments. For eager execution, pass
<code>run_eagerly=True</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of invalid arguments for
<code>optimizer</code>, <code>loss</code>, <code>metrics</code> or <code>sample_weight_mode</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self,
  optimizer=&#39;rmsprop&#39;,
  loss=None,
  metrics=None,
  loss_weights=None,
  weighted_metrics=None,
  run_eagerly=None,
  **kwargs):

  self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
  self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
  self.decoder.compile(optimizer = copy.deepcopy(optimizer))
  self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))

  super().compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics,
    loss_weights=loss_weights,
    weighted_metrics=weighted_metrics,
    run_eagerly=run_eagerly,
    **kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates output predictions for the input samples.</p>
<p>Computation is done in batches. This method is designed for performance in
large scale inputs. For small amount of inputs that fit in one batch,
directly using <code>__call__</code> is recommended for faster execution, e.g.,
<code>model(x)</code>, or <code>model(x, training=False)</code> if you have layers such as
<code>tf.keras.layers.BatchNormalization</code> that behaves differently during
inference.</p>
<h2 id="arguments">Arguments</h2>
<p>x: Input samples. It could be:
- A Numpy array (or array-like), or a list of arrays
(in case the model has multiple inputs).
- A TensorFlow tensor, or a list of tensors
(in case the model has multiple inputs).
- A <code>tf.data</code> dataset.
- A generator or <code>keras.utils.Sequence</code> instance.
A more detailed description of unpacking behavior for iterator types
(Dataset, generator, Sequence) is given in the <code>Unpacking behavior
for iterator-like inputs&lt;code&gt; section of &lt;/code&gt;Model.fit</code>.
batch_size: Integer or <code>None</code>.
Number of samples per batch.
If unspecified, <code>batch_size</code> will default to 32.
Do not specify the <code>batch_size</code> if your data is in the
form of dataset, generators, or <code>keras.utils.Sequence</code> instances
(since they generate batches).
verbose: Verbosity mode, 0 or 1.
steps: Total number of steps (batches of samples)
before declaring the prediction round finished.
Ignored with the default value of <code>None</code>. If x is a <code>tf.data</code>
dataset and <code>steps</code> is None, <code>predict</code> will
run until the input dataset is exhausted.
callbacks: List of <code>keras.callbacks.Callback</code> instances.
List of callbacks to apply during prediction.
See <a href="/api_docs/python/tf/keras/callbacks">callbacks</a>.
max_queue_size: Integer. Used for generator or <code>keras.utils.Sequence</code>
input only. Maximum size for the generator queue.
If unspecified, <code>max_queue_size</code> will default to 10.
workers: Integer. Used for generator or <code>keras.utils.Sequence</code> input
only. Maximum number of processes to spin up when using
process-based threading. If unspecified, <code>workers</code> will default
to 1. If 0, will execute the generator on the main thread.
use_multiprocessing: Boolean. Used for generator or
<code>keras.utils.Sequence</code> input only. If <code>True</code>, use process-based
threading. If unspecified, <code>use_multiprocessing</code> will default to
<code>False</code>. Note that because this implementation relies on
multiprocessing, you should not pass non-picklable arguments to
the generator as they can't be passed easily to children processes.</p>
<p>See the discussion of <code>Unpacking behavior for iterator-like inputs</code> for
<code>Model.fit</code>. Note that Model.predict uses the same interpretation rules as
<code>Model.fit</code> and <code>Model.evaluate</code>, so inputs must be unambiguous for all
three methods.</p>
<h2 id="returns">Returns</h2>
<p>Numpy array(s) of predictions.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of mismatch between the provided
input data and the model's expectations,
or in case a stateful model receives a number of samples
that is not a multiple of the batch size.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, *args, **kwargs):
  return self.image_encoder.predict(*args,**kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.prior_discriminator_loss"><code class="name flex">
<span>def <span class="ident">prior_discriminator_loss</span></span>(<span>self, real, fake)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prior_discriminator_loss(self,real,fake):
  real_loss = self.cross_entropy(tf.ones_like(real), real)
  fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
  return (real_loss + fake_loss)/(2*self.prior_batch_size)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, output_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to an output folder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target output folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self,output_dir: str):
  &#34;&#34;&#34;Save the model to an output folder
  
  Args:
      output_dir (str): Target output folder
  &#34;&#34;&#34;
  self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
  self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
  self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
  self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

  d = {
    &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
    &#34;prior_batch_size&#34;: self.prior_batch_size
  }

  (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
  #   json.dump(d,f)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.CharStyleSAAE.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <code>Model.make_train_function</code>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <code>tf.function</code> and
<code>tf.distribute.Strategy</code> settings), should be left to
<code>Model.make_train_function</code>, which can also be overridden.</p>
<h2 id="arguments">Arguments</h2>
<p>data: A nested structure of <code>Tensor</code>s.</p>
<h2 id="returns">Returns</h2>
<p>A <code>dict</code> containing values that will be passed to
<code>tf.keras.callbacks.CallbackList.on_train_batch_end</code>. Typically, the
values of the <code>Model</code>'s metrics are returned. Example:
<code>{'loss': 0.2, 'accuracy': 0.7}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(self, inputs):
  x, labels = inputs

  #self.prior_batch_size = x.shape[0]
  #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

  with tf.GradientTape(persistent=True) as tape:

    # apply autoencoder
    image_precode = self.image_encoder(x, training=True)
    full_precode = tf.concat([image_precode, labels], axis=-1)
    code = self.full_encoder(full_precode, training=True)
    extended_code = tf.concat([code,labels],axis=-1)
    decoded = self.decoder(extended_code,training=True)  

    # apply prior_discriminator model
    #print((self.prior_batch_size,code.shape[1]))
    prior_samples = self.prior_sampler(shape=[self.prior_batch_size,code.shape[1]])
    real = self.prior_discriminator(prior_samples,training=True)
    fake = self.prior_discriminator(code,training=True)

    # compute losses for the models
    reconstruction_loss = tf.keras.losses.MSE(x,decoded)
    classification_loss = self.prior_discriminator_loss(real,fake)
    mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

  # Compute gradients
  discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
  decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
  image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
  full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)


  #apply gradients
  self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
  self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
  self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
  self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

  # compute metrics
  self.mse_metric.update_state(x,decoded)

  discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
  discr_predicted = tf.concat([real,fake],axis=0)
  self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  #self.cross_entropy_metric.update_state(discr_true, discr_predicted)

  return {&#34;reconstruction MSE&#34;: self.mse_metric.result(), &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE"><code class="flex name class">
<span>class <span class="ident">PureCharStyleSAAE</span></span>
<span>(</span><span>full_encoder: tensorflow.python.keras.engine.training.Model, image_encoder: tensorflow.python.keras.engine.training.Model, decoder: tensorflow.python.keras.engine.training.Model, char_discriminator: tensorflow.python.keras.engine.training.Model, prior_discriminator: tensorflow.python.keras.engine.training.Model, reconstruction_loss_weight: float = 0.5, prior_batch_size: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>This model is trained as a regular SAAE but an additional discriminator model is added to ensure the embedding does not retain information about the character class; i.e. it only retains style information</p>
<p>Summary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>char_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator to remove any character information from embeddings</dd>
<dt><strong><code>image_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>reconstruction_loss_weight</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Weight of reconstruction loss at training time. Should be between 0 and 1.</dd>
<dt><strong><code>n_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>number of labeled classes</dd>
<dt><strong><code>prior_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size from prior distribution at training time</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PureCharStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model is trained as a regular SAAE but an additional discriminator model is added to ensure the embedding does not retain information about the character class; i.e. it only retains style information
  &#34;&#34;&#34;
  mean_metric = tf.keras.metrics.Mean(name=&#34;Mean code variance&#34;)
  char_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;style-char accuracy&#34;)
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)


  style_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)
  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    char_discriminator: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        char_discriminator (tf.keras.Model): Discriminator to remove any character information from embeddings
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(PureCharStyleSAAE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size
    self.char_discriminator = char_discriminator

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;, &#34;char_discriminator&#34;]

  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
    self.decoder.compile(optimizer = copy.deepcopy(optimizer))
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))
    self.char_discriminator.compile(optimizer=copy.deepcopy(optimizer))

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(x, training=True)
      full_precode = tf.concat([image_precode, labels], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,labels],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(self.prior_batch_size,code.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)

      # apply char_discriminator model
      char_guess = self.char_discriminator(code,training=True)

      # compute losses for the models
      char_loss = self.style_loss(labels, char_guess)/self.prior_batch_size
      reconstruction_loss = tf.keras.losses.MSE(x,decoded)
      classification_loss = self.prior_discriminator_loss(real,fake)

      mixed_loss = -(1-self.rec_loss_weight)*(classification_loss + char_loss) + self.rec_loss_weight*(reconstruction_loss)

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
    style_discr_gradients = tape.gradient(char_loss, self.char_discriminator.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
    self.char_discriminator.optimizer.apply_gradients(
      zip(style_discr_gradients, self.char_discriminator.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    #self.cross_entropy_metric.update_state(discr_true, discr_predicted)


    self.char_accuracy_metric.update_state(tf.argmax(labels, axis=-1), tf.argmax(char_guess, axis=-1))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result(), 
    &#34;sstyle discriminator accuracy&#34;: self.char_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.char_discriminator.save(str(BytestreamPath(output_dir) / &#34;char_discriminator&#34;))
    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size
    }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)
    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    char_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;char_discriminator&#34;))
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())

    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      char_discriminator = char_discriminator,
      **d)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.network.Network</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.prediction.models.PureCharStyleSAAE.char_accuracy_metric"><code class="name">var <span class="ident">char_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.cross_entropy"><code class="name">var <span class="ident">cross_entropy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.cross_entropy_metric"><code class="name">var <span class="ident">cross_entropy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.mean_metric"><code class="name">var <span class="ident">mean_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.mse_metric"><code class="name">var <span class="ident">mse_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.prior_accuracy_metric"><code class="name">var <span class="ident">prior_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.style_loss"><code class="name">var <span class="ident">style_loss</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.prediction.models.PureCharStyleSAAE.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>input_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a saved instance of this class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target input folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SAAE</code></dt>
<dd>Loaded model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, input_dir: str):
  &#34;&#34;&#34;Loads a saved instance of this class
  
  Args:
      input_dir (str): Target input folder
  
  Returns:
      SAAE: Loaded model
  &#34;&#34;&#34;
  char_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;char_discriminator&#34;))
  full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
  image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
  decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
  prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

  # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
  #   d = json.loads(f.read())

  d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
  d = json.loads(d_string)

  return cls(
    image_encoder = image_encoder, 
    full_encoder = full_encoder, 
    decoder = decoder, 
    prior_discriminator = prior_discriminator, 
    char_discriminator = char_discriminator,
    **d)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.models.PureCharStyleSAAE.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the model for training.</p>
<h2 id="arguments">Arguments</h2>
<p>optimizer: String (name of optimizer) or optimizer instance.
See <code>tf.keras.optimizers</code>.
loss: String (name of objective function), objective function or
<code>tf.keras.losses.Loss</code> instance. See <code>tf.keras.losses</code>.
An objective function is any callable with the signature
<code>loss = fn(y_true, y_pred)</code>, where
y_true = ground truth values with shape = <code>[batch_size, d0, .. dN]</code>,
except sparse loss functions such as sparse categorical crossentropy
where shape = <code>[batch_size, d0, .. dN-1]</code>.
y_pred = predicted values with shape = <code>[batch_size, d0, .. dN]</code>.
It returns a weighted loss float tensor.
If a custom <code>Loss</code> instance is used and reduction is set to NONE,
return value has the shape [batch_size, d0, .. dN-1] ie. per-sample
or per-timestep loss values; otherwise, it is a scalar.
If the model has multiple outputs, you can use a different loss on
each output by passing a dictionary or a list of losses. The loss
value that will be minimized by the model will then be the sum of
all individual losses.
metrics: List of metrics to be evaluated by the model during training
and testing.
Each of this can be a string (name of a built-in function), function
or a <code>tf.keras.metrics.Metric</code> instance. See <code>tf.keras.metrics</code>.
Typically you will use <code>metrics=['accuracy']</code>. A function is any
callable with the signature <code>result = fn(y_true, y_pred)</code>.
To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<code>metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}</code>.
You can also pass a list (len = len(outputs)) of lists of metrics
such as <code>metrics=[['accuracy'], ['accuracy', 'mse']]</code> or
<code>metrics=['accuracy', ['accuracy', 'mse']]</code>.
When you pass the strings 'accuracy' or 'acc', we convert this to
one of <code>tf.keras.metrics.BinaryAccuracy</code>,
<code>tf.keras.metrics.CategoricalAccuracy</code>,
<code>tf.keras.metrics.SparseCategoricalAccuracy</code> based on the loss
function used and the model output shape. We do a similar conversion
for the strings 'crossentropy' and 'ce' as well.
loss_weights: Optional list or dictionary specifying scalar
coefficients (Python floats) to weight the loss contributions
of different model outputs.
The loss value that will be minimized by the model
will then be the <em>weighted sum</em> of all individual losses,
weighted by the <code>loss_weights</code> coefficients.
If a list, it is expected to have a 1:1 mapping
to the model's outputs. If a dict, it is expected to map
output names (strings) to scalar coefficients.
sample_weight_mode: If you need to do timestep-wise
sample weighting (2D weights), set this to <code>"temporal"</code>.
<code>None</code> defaults to sample-wise weights (1D).
If the model has multiple outputs, you can use a different
<code>sample_weight_mode</code> on each output by passing a
dictionary or a list of modes.
weighted_metrics: List of metrics to be evaluated and weighted
by sample_weight or class_weight during training and testing.
**kwargs: Any additional arguments. For eager execution, pass
<code>run_eagerly=True</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of invalid arguments for
<code>optimizer</code>, <code>loss</code>, <code>metrics</code> or <code>sample_weight_mode</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self,
  optimizer=&#39;rmsprop&#39;,
  loss=None,
  metrics=None,
  loss_weights=None,
  weighted_metrics=None,
  run_eagerly=None,
  **kwargs):

  self.full_encoder.compile(optimizer = copy.deepcopy(optimizer))
  self.image_encoder.compile(optimizer = copy.deepcopy(optimizer))
  self.decoder.compile(optimizer = copy.deepcopy(optimizer))
  self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer))
  self.char_discriminator.compile(optimizer=copy.deepcopy(optimizer))

  super().compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics,
    loss_weights=loss_weights,
    weighted_metrics=weighted_metrics,
    run_eagerly=run_eagerly,
    **kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.prior_discriminator_loss"><code class="name flex">
<span>def <span class="ident">prior_discriminator_loss</span></span>(<span>self, real, fake)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prior_discriminator_loss(self,real,fake):
  real_loss = self.cross_entropy(tf.ones_like(real), real)
  fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
  return (real_loss + fake_loss)/(2*self.prior_batch_size)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, output_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to an output folder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target output folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self,output_dir: str):
  &#34;&#34;&#34;Save the model to an output folder
  
  Args:
      output_dir (str): Target output folder
  &#34;&#34;&#34;

  self.char_discriminator.save(str(BytestreamPath(output_dir) / &#34;char_discriminator&#34;))
  self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
  self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
  self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
  self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

  d = {
    &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
    &#34;prior_batch_size&#34;: self.prior_batch_size
  }

  # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
  #   json.dump(d,f)
  (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureCharStyleSAAE.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <code>Model.make_train_function</code>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <code>tf.function</code> and
<code>tf.distribute.Strategy</code> settings), should be left to
<code>Model.make_train_function</code>, which can also be overridden.</p>
<h2 id="arguments">Arguments</h2>
<p>data: A nested structure of <code>Tensor</code>s.</p>
<h2 id="returns">Returns</h2>
<p>A <code>dict</code> containing values that will be passed to
<code>tf.keras.callbacks.CallbackList.on_train_batch_end</code>. Typically, the
values of the <code>Model</code>'s metrics are returned. Example:
<code>{'loss': 0.2, 'accuracy': 0.7}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(self, inputs):
  #prior_sampler = tf.random.normal
  x, labels = inputs

  #self.prior_batch_size = x.shape[0]
  #logger.info(&#34;prior_batch_size is deprecated; setting it equal to batch size.&#34;)

  with tf.GradientTape(persistent=True) as tape:

    # apply autoencoder
    image_precode = self.image_encoder(x, training=True)
    full_precode = tf.concat([image_precode, labels], axis=-1)
    code = self.full_encoder(full_precode, training=True)
    extended_code = tf.concat([code,labels],axis=-1)
    decoded = self.decoder(extended_code,training=True)  

    # apply prior_discriminator model
    prior_samples = self.prior_sampler(shape=(self.prior_batch_size,code.shape[1]))
    real = self.prior_discriminator(prior_samples,training=True)
    fake = self.prior_discriminator(code,training=True)

    # apply char_discriminator model
    char_guess = self.char_discriminator(code,training=True)

    # compute losses for the models
    char_loss = self.style_loss(labels, char_guess)/self.prior_batch_size
    reconstruction_loss = tf.keras.losses.MSE(x,decoded)
    classification_loss = self.prior_discriminator_loss(real,fake)

    mixed_loss = -(1-self.rec_loss_weight)*(classification_loss + char_loss) + self.rec_loss_weight*(reconstruction_loss)

  # Compute gradients
  discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
  decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
  image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
  full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
  style_discr_gradients = tape.gradient(char_loss, self.char_discriminator.trainable_variables)

  #apply gradients
  self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
  self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
  self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
  self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
  self.char_discriminator.optimizer.apply_gradients(
    zip(style_discr_gradients, self.char_discriminator.trainable_variables))

  # compute metrics
  self.mse_metric.update_state(x,decoded)

  discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
  discr_predicted = tf.concat([real,fake],axis=0)
  self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  #self.cross_entropy_metric.update_state(discr_true, discr_predicted)


  self.char_accuracy_metric.update_state(tf.argmax(labels, axis=-1), tf.argmax(char_guess, axis=-1))

  return {
  &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
  &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result(), 
  &#34;sstyle discriminator accuracy&#34;: self.char_accuracy_metric.result()}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE"><code class="flex name class">
<span>class <span class="ident">PureFontStyleSA2AE</span></span>
<span>(</span><span>full_encoder: tensorflow.python.keras.engine.training.Model, image_encoder: tensorflow.python.keras.engine.training.Model, decoder: tensorflow.python.keras.engine.training.Model, prior_discriminator: tensorflow.python.keras.engine.training.Model, image_discriminator: tensorflow.python.keras.engine.training.Model, code_regularisation_weight=0)</span>
</code></dt>
<dd>
<div class="desc"><p>This model is works like PureFontStyleSA2AE but instead of minimising the MSE reconstruction error, an additional image discriminator classify between real and reconstructed images, so the encoder and decoder now maximise misclassification error.</p>
<p>Summary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>image_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>image_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>image discriminator</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PureFontStyleSA2AE(tf.keras.Model):

  &#34;&#34;&#34;This model is works like PureFontStyleSA2AE but instead of minimising the MSE reconstruction error, an additional image discriminator classify between real and reconstructed images, so the encoder and decoder now maximise misclassification error.

  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;adversarial prior accuracy&#34;)
  image_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;adversarial image accuracy&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    image_discriminator: tf.keras.Model,
    code_regularisation_weight=0):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        image_discriminator (tf.keras.Model): image discriminator
    &#34;&#34;&#34;
    super(PureFontStyleSA2AE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.image_discriminator = image_discriminator

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;, &#34;image_discriminator&#34;]


  def discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
    &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
    
    Args:
        x (tf.Tensor): Feature tensor
        labels (tf.Tensor): Label tensor
    
    Returns:
        t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
    &#34;&#34;&#34;
    #
    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    #
    style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

    outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    #
    for k in range(n_fonts):
      x_k, labels_k = x[k], labels[k]
      x_k_shape = tf.shape(x_k)
      shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
      scrambled = tf.random.shuffle(shuffling_idx)
      #
      style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
      style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

      outcome_x = outcome_x.write(k, x_k)
      outcome_y = outcome_y.write(k, labels_k)
      #
    #
    #
    return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

    n_examples = tf.shape(style_x)[0]
    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(style_x, training=True)
      full_precode = tf.concat([image_precode, style_y], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,outcome_y],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
      real_prior = self.prior_discriminator(prior_samples,training=True)
      fake_prior = self.prior_discriminator(code,training=True)
      prior_classification_loss = self.discriminator_loss(real_prior,fake_prior)

      #apply image_discriminator model
      real_image = self.image_discriminator(outcome_x)
      fake_image = self.image_discriminator(decoded)
      image_classification_loss = self.discriminator_loss(real_image, fake_image)

      # compute losses for the models
      #reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
      
      mixed_loss = -(prior_classification_loss + image_classification_loss)

      #mixed_loss = -(1-self.rec_loss_weight)*(prior_classification_loss) + self.rec_loss_weight*(reconstruction_loss)


    # Compute gradients
    prior_discriminator_gradients = tape.gradient(prior_classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(image_classification_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
    image_discriminator_gradients = tape.gradient(image_classification_loss, self.image_discriminator.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(prior_discriminator_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
    self.image_discriminator.optimizer.apply_gradients(zip(image_discriminator_gradients, self.image_discriminator.trainable_variables))

    # compute metrics
    #self.mse_metric.update_state(outcome_x,decoded)

    discr_true = tf.concat([tf.ones_like(real_prior),tf.zeros_like(fake_prior)],axis=0)
    discr_predicted = tf.concat([real_prior,fake_prior],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    discr_true = tf.concat([tf.ones_like(real_image),tf.zeros_like(fake_image)],axis=0)
    discr_predicted = tf.concat([real_image,fake_image],axis=0)
    self.image_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;image discriminator accuracy&#34;: self.image_accuracy_metric.result(), 
    &#34;prior discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))
    self.image_discriminator.save(str(BytestreamPath(output_dir) / &#34;image_discriminator&#34;))


    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))
    image_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_discriminator&#34;))

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      image_discriminator = image_discriminator)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.network.Network</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.cross_entropy"><code class="name">var <span class="ident">cross_entropy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.cross_entropy_metric"><code class="name">var <span class="ident">cross_entropy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.image_accuracy_metric"><code class="name">var <span class="ident">image_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.prior_accuracy_metric"><code class="name">var <span class="ident">prior_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>input_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a saved instance of this class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target input folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SAAE</code></dt>
<dd>Loaded model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, input_dir: str):
  &#34;&#34;&#34;Loads a saved instance of this class
  
  Args:
      input_dir (str): Target input folder
  
  Returns:
      SAAE: Loaded model
  &#34;&#34;&#34;
  full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
  image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
  decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
  prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))
  image_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_discriminator&#34;))

  return cls(
    image_encoder = image_encoder, 
    full_encoder = full_encoder, 
    decoder = decoder, 
    prior_discriminator = prior_discriminator, 
    image_discriminator = image_discriminator)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the model for training.</p>
<h2 id="arguments">Arguments</h2>
<p>optimizer: String (name of optimizer) or optimizer instance.
See <code>tf.keras.optimizers</code>.
loss: String (name of objective function), objective function or
<code>tf.keras.losses.Loss</code> instance. See <code>tf.keras.losses</code>.
An objective function is any callable with the signature
<code>loss = fn(y_true, y_pred)</code>, where
y_true = ground truth values with shape = <code>[batch_size, d0, .. dN]</code>,
except sparse loss functions such as sparse categorical crossentropy
where shape = <code>[batch_size, d0, .. dN-1]</code>.
y_pred = predicted values with shape = <code>[batch_size, d0, .. dN]</code>.
It returns a weighted loss float tensor.
If a custom <code>Loss</code> instance is used and reduction is set to NONE,
return value has the shape [batch_size, d0, .. dN-1] ie. per-sample
or per-timestep loss values; otherwise, it is a scalar.
If the model has multiple outputs, you can use a different loss on
each output by passing a dictionary or a list of losses. The loss
value that will be minimized by the model will then be the sum of
all individual losses.
metrics: List of metrics to be evaluated by the model during training
and testing.
Each of this can be a string (name of a built-in function), function
or a <code>tf.keras.metrics.Metric</code> instance. See <code>tf.keras.metrics</code>.
Typically you will use <code>metrics=['accuracy']</code>. A function is any
callable with the signature <code>result = fn(y_true, y_pred)</code>.
To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<code>metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}</code>.
You can also pass a list (len = len(outputs)) of lists of metrics
such as <code>metrics=[['accuracy'], ['accuracy', 'mse']]</code> or
<code>metrics=['accuracy', ['accuracy', 'mse']]</code>.
When you pass the strings 'accuracy' or 'acc', we convert this to
one of <code>tf.keras.metrics.BinaryAccuracy</code>,
<code>tf.keras.metrics.CategoricalAccuracy</code>,
<code>tf.keras.metrics.SparseCategoricalAccuracy</code> based on the loss
function used and the model output shape. We do a similar conversion
for the strings 'crossentropy' and 'ce' as well.
loss_weights: Optional list or dictionary specifying scalar
coefficients (Python floats) to weight the loss contributions
of different model outputs.
The loss value that will be minimized by the model
will then be the <em>weighted sum</em> of all individual losses,
weighted by the <code>loss_weights</code> coefficients.
If a list, it is expected to have a 1:1 mapping
to the model's outputs. If a dict, it is expected to map
output names (strings) to scalar coefficients.
sample_weight_mode: If you need to do timestep-wise
sample weighting (2D weights), set this to <code>"temporal"</code>.
<code>None</code> defaults to sample-wise weights (1D).
If the model has multiple outputs, you can use a different
<code>sample_weight_mode</code> on each output by passing a
dictionary or a list of modes.
weighted_metrics: List of metrics to be evaluated and weighted
by sample_weight or class_weight during training and testing.
**kwargs: Any additional arguments. For eager execution, pass
<code>run_eagerly=True</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of invalid arguments for
<code>optimizer</code>, <code>loss</code>, <code>metrics</code> or <code>sample_weight_mode</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self,
  optimizer=&#39;rmsprop&#39;,
  loss=None,
  metrics=None,
  loss_weights=None,
  weighted_metrics=None,
  run_eagerly=None,
  **kwargs):

  self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.image_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

  super().compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics,
    loss_weights=loss_weights,
    weighted_metrics=weighted_metrics,
    run_eagerly=run_eagerly,
    **kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.discriminator_loss"><code class="name flex">
<span>def <span class="ident">discriminator_loss</span></span>(<span>self, real, fake)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def discriminator_loss(self,real,fake):
  real_loss = self.cross_entropy(tf.ones_like(real), real)
  fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
  return (real_loss + fake_loss)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, output_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to an output folder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target output folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self,output_dir: str):
  &#34;&#34;&#34;Save the model to an output folder
  
  Args:
      output_dir (str): Target output folder
  &#34;&#34;&#34;

  self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
  self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
  self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
  self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))
  self.image_discriminator.save(str(BytestreamPath(output_dir) / &#34;image_discriminator&#34;))


  # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
  #   json.dump(d,f)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.scramble_font_batches"><code class="name flex">
<span>def <span class="ident">scramble_font_batches</span></span>(<span>self, x: tensorflow.python.framework.ops.Tensor, labels: tensorflow.python.framework.ops.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>Feature tensor</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>Label tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]</code></dt>
<dd>return scrambled and original feature-label pairs, in that order.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
  &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
  
  Args:
      x (tf.Tensor): Feature tensor
      labels (tf.Tensor): Label tensor
  
  Returns:
      t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
  &#34;&#34;&#34;
  #
  x_shape = tf.shape(x)
  n_fonts = x_shape[0]
  #
  style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

  outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  #
  for k in range(n_fonts):
    x_k, labels_k = x[k], labels[k]
    x_k_shape = tf.shape(x_k)
    shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
    scrambled = tf.random.shuffle(shuffling_idx)
    #
    style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
    style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

    outcome_x = outcome_x.write(k, x_k)
    outcome_y = outcome_y.write(k, labels_k)
    #
  #
  #
  return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSA2AE.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <code>Model.make_train_function</code>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <code>tf.function</code> and
<code>tf.distribute.Strategy</code> settings), should be left to
<code>Model.make_train_function</code>, which can also be overridden.</p>
<h2 id="arguments">Arguments</h2>
<p>data: A nested structure of <code>Tensor</code>s.</p>
<h2 id="returns">Returns</h2>
<p>A <code>dict</code> containing values that will be passed to
<code>tf.keras.callbacks.CallbackList.on_train_batch_end</code>. Typically, the
values of the <code>Model</code>'s metrics are returned. Example:
<code>{'loss': 0.2, 'accuracy': 0.7}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(self, inputs):
  #prior_sampler = tf.random.normal
  x, labels = inputs

  #self.prior_batch_size = x.shape[0]
  style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

  n_examples = tf.shape(style_x)[0]
  with tf.GradientTape(persistent=True) as tape:

    # apply autoencoder
    image_precode = self.image_encoder(style_x, training=True)
    full_precode = tf.concat([image_precode, style_y], axis=-1)
    code = self.full_encoder(full_precode, training=True)
    extended_code = tf.concat([code,outcome_y],axis=-1)
    decoded = self.decoder(extended_code,training=True)  

    # apply prior_discriminator model
    prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
    real_prior = self.prior_discriminator(prior_samples,training=True)
    fake_prior = self.prior_discriminator(code,training=True)
    prior_classification_loss = self.discriminator_loss(real_prior,fake_prior)

    #apply image_discriminator model
    real_image = self.image_discriminator(outcome_x)
    fake_image = self.image_discriminator(decoded)
    image_classification_loss = self.discriminator_loss(real_image, fake_image)

    # compute losses for the models
    #reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
    
    mixed_loss = -(prior_classification_loss + image_classification_loss)

    #mixed_loss = -(1-self.rec_loss_weight)*(prior_classification_loss) + self.rec_loss_weight*(reconstruction_loss)


  # Compute gradients
  prior_discriminator_gradients = tape.gradient(prior_classification_loss,self.prior_discriminator.trainable_variables)
  decoder_gradients = tape.gradient(image_classification_loss, self.decoder.trainable_variables)
  image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
  full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)
  image_discriminator_gradients = tape.gradient(image_classification_loss, self.image_discriminator.trainable_variables)

  #apply gradients
  self.prior_discriminator.optimizer.apply_gradients(zip(prior_discriminator_gradients,self.prior_discriminator.trainable_variables))
  self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
  self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
  self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))
  self.image_discriminator.optimizer.apply_gradients(zip(image_discriminator_gradients, self.image_discriminator.trainable_variables))

  # compute metrics
  #self.mse_metric.update_state(outcome_x,decoded)

  discr_true = tf.concat([tf.ones_like(real_prior),tf.zeros_like(fake_prior)],axis=0)
  discr_predicted = tf.concat([real_prior,fake_prior],axis=0)
  self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  discr_true = tf.concat([tf.ones_like(real_image),tf.zeros_like(fake_image)],axis=0)
  discr_predicted = tf.concat([real_image,fake_image],axis=0)
  self.image_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  return {
  &#34;image discriminator accuracy&#34;: self.image_accuracy_metric.result(), 
  &#34;prior discriminator accuracy&#34;: self.prior_accuracy_metric.result()}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE"><code class="flex name class">
<span>class <span class="ident">PureFontStyleSAAE</span></span>
<span>(</span><span>full_encoder: tensorflow.python.keras.engine.training.Model, image_encoder: tensorflow.python.keras.engine.training.Model, decoder: tensorflow.python.keras.engine.training.Model, prior_discriminator: tensorflow.python.keras.engine.training.Model, reconstruction_loss_weight: float = 0.5, prior_batch_size: int = 32, code_regularisation_weight=0)</span>
</code></dt>
<dd>
<div class="desc"><p>This model is trained on all characters from one or more font files at a time; the aim is to encode the font's style as opposed to single characters' styles, which can happen when training with scrambled characters from different fonts and results in sometimes having different-looking image styles for a given style in latent space. This model works with characters from a single typeface at a time, and use the style from a given character to decode a different randomly chosen character in the same font, using the encoded style and target label information. </p>
<p>Summary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>image_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>reconstruction_loss_weight</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Weight of reconstruction loss at training time. Should be between 0 and 1.</dd>
<dt><strong><code>n_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>number of labeled classes</dd>
<dt><strong><code>prior_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size from prior distribution at training time</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PureFontStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model is trained on all characters from one or more font files at a time; the aim is to encode the font&#39;s style as opposed to single characters&#39; styles, which can happen when training with scrambled characters from different fonts and results in sometimes having different-looking image styles for a given style in latent space. This model works with characters from a single typeface at a time, and use the style from a given character to decode a different randomly chosen character in the same font, using the encoded style and target label information. 
  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    full_encoder: tf.keras.Model,
    image_encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5,
    prior_batch_size:int=32,
    code_regularisation_weight=0):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        image_encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(PureFontStyleSAAE, self).__init__()

    self.full_encoder = full_encoder
    self.image_encoder = image_encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)
    self.prior_batch_size = prior_batch_size

    self.prior_sampler = tf.random.normal
    self.code_regularisation_weight = code_regularisation_weight
    # list of embedded models as instance attributes 
    self.model_list = [&#34;full_encoder&#34;, &#34;image_encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]


  def prior_discriminator_loss(self,real,fake):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/(2*self.prior_batch_size)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.image_encoder(x, training=training)

  def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
    &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
    
    Args:
        x (tf.Tensor): Feature tensor
        labels (tf.Tensor): Label tensor
    
    Returns:
        t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
    &#34;&#34;&#34;
    #
    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    #
    style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

    outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
    #
    for k in range(n_fonts):
      x_k, labels_k = x[k], labels[k]
      x_k_shape = tf.shape(x_k)
      shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
      scrambled = tf.random.shuffle(shuffling_idx)
      #
      style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
      style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

      outcome_x = outcome_x.write(k, x_k)
      outcome_y = outcome_y.write(k, labels_k)
      #
    #
    #
    return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, labels = inputs

    #self.prior_batch_size = x.shape[0]
    style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

    n_examples = tf.shape(style_x)[0]
    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      image_precode = self.image_encoder(style_x, training=True)
      full_precode = tf.concat([image_precode, style_y], axis=-1)
      code = self.full_encoder(full_precode, training=True)
      extended_code = tf.concat([code,outcome_y],axis=-1)
      decoded = self.decoder(extended_code,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(code,training=True)


      # Moment regularization for embedded representation to keep it closer to standard normal
      reg = self.code_regularisation_weight*(tf.reduce_mean(code)**2 + (tf.reduce_mean(code**2) - 1.0)**2)/2

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
      classification_loss = self.prior_discriminator_loss(real,fake)

      mixed_loss = -(1-self.rec_loss_weight)*(classification_loss) + self.rec_loss_weight*(reconstruction_loss + reg)


    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
    full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
    self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(outcome_x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
    self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
      &#34;prior_batch_size&#34;: self.prior_batch_size,
      &#34;code_regularisation_weight&#34;: self.code_regularisation_weight
    }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
    image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())
    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      image_encoder = image_encoder, 
      full_encoder = full_encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.network.Network</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSAAE.cross_entropy"><code class="name">var <span class="ident">cross_entropy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.cross_entropy_metric"><code class="name">var <span class="ident">cross_entropy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.mse_metric"><code class="name">var <span class="ident">mse_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.prior_accuracy_metric"><code class="name">var <span class="ident">prior_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSAAE.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>input_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a saved instance of this class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target input folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SAAE</code></dt>
<dd>Loaded model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, input_dir: str):
  &#34;&#34;&#34;Loads a saved instance of this class
  
  Args:
      input_dir (str): Target input folder
  
  Returns:
      SAAE: Loaded model
  &#34;&#34;&#34;
  full_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;full_encoder&#34;))
  image_encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;image_encoder&#34;))
  decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
  prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

  # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
  #   d = json.loads(f.read())
  d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
  d = json.loads(d_string)

  return cls(
    image_encoder = image_encoder, 
    full_encoder = full_encoder, 
    decoder = decoder, 
    prior_discriminator = prior_discriminator, 
    **d)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.models.PureFontStyleSAAE.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the model for training.</p>
<h2 id="arguments">Arguments</h2>
<p>optimizer: String (name of optimizer) or optimizer instance.
See <code>tf.keras.optimizers</code>.
loss: String (name of objective function), objective function or
<code>tf.keras.losses.Loss</code> instance. See <code>tf.keras.losses</code>.
An objective function is any callable with the signature
<code>loss = fn(y_true, y_pred)</code>, where
y_true = ground truth values with shape = <code>[batch_size, d0, .. dN]</code>,
except sparse loss functions such as sparse categorical crossentropy
where shape = <code>[batch_size, d0, .. dN-1]</code>.
y_pred = predicted values with shape = <code>[batch_size, d0, .. dN]</code>.
It returns a weighted loss float tensor.
If a custom <code>Loss</code> instance is used and reduction is set to NONE,
return value has the shape [batch_size, d0, .. dN-1] ie. per-sample
or per-timestep loss values; otherwise, it is a scalar.
If the model has multiple outputs, you can use a different loss on
each output by passing a dictionary or a list of losses. The loss
value that will be minimized by the model will then be the sum of
all individual losses.
metrics: List of metrics to be evaluated by the model during training
and testing.
Each of this can be a string (name of a built-in function), function
or a <code>tf.keras.metrics.Metric</code> instance. See <code>tf.keras.metrics</code>.
Typically you will use <code>metrics=['accuracy']</code>. A function is any
callable with the signature <code>result = fn(y_true, y_pred)</code>.
To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<code>metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}</code>.
You can also pass a list (len = len(outputs)) of lists of metrics
such as <code>metrics=[['accuracy'], ['accuracy', 'mse']]</code> or
<code>metrics=['accuracy', ['accuracy', 'mse']]</code>.
When you pass the strings 'accuracy' or 'acc', we convert this to
one of <code>tf.keras.metrics.BinaryAccuracy</code>,
<code>tf.keras.metrics.CategoricalAccuracy</code>,
<code>tf.keras.metrics.SparseCategoricalAccuracy</code> based on the loss
function used and the model output shape. We do a similar conversion
for the strings 'crossentropy' and 'ce' as well.
loss_weights: Optional list or dictionary specifying scalar
coefficients (Python floats) to weight the loss contributions
of different model outputs.
The loss value that will be minimized by the model
will then be the <em>weighted sum</em> of all individual losses,
weighted by the <code>loss_weights</code> coefficients.
If a list, it is expected to have a 1:1 mapping
to the model's outputs. If a dict, it is expected to map
output names (strings) to scalar coefficients.
sample_weight_mode: If you need to do timestep-wise
sample weighting (2D weights), set this to <code>"temporal"</code>.
<code>None</code> defaults to sample-wise weights (1D).
If the model has multiple outputs, you can use a different
<code>sample_weight_mode</code> on each output by passing a
dictionary or a list of modes.
weighted_metrics: List of metrics to be evaluated and weighted
by sample_weight or class_weight during training and testing.
**kwargs: Any additional arguments. For eager execution, pass
<code>run_eagerly=True</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of invalid arguments for
<code>optimizer</code>, <code>loss</code>, <code>metrics</code> or <code>sample_weight_mode</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self,
  optimizer=&#39;rmsprop&#39;,
  loss=None,
  metrics=None,
  loss_weights=None,
  weighted_metrics=None,
  run_eagerly=None,
  **kwargs):

  self.full_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.image_encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

  super().compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics,
    loss_weights=loss_weights,
    weighted_metrics=weighted_metrics,
    run_eagerly=run_eagerly,
    **kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.prior_discriminator_loss"><code class="name flex">
<span>def <span class="ident">prior_discriminator_loss</span></span>(<span>self, real, fake)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prior_discriminator_loss(self,real,fake):
  real_loss = self.cross_entropy(tf.ones_like(real), real)
  fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
  return (real_loss + fake_loss)/(2*self.prior_batch_size)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, output_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to an output folder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target output folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self,output_dir: str):
  &#34;&#34;&#34;Save the model to an output folder
  
  Args:
      output_dir (str): Target output folder
  &#34;&#34;&#34;

  self.full_encoder.save(str(BytestreamPath(output_dir) / &#34;full_encoder&#34;))
  self.image_encoder.save(str(BytestreamPath(output_dir) / &#34;image_encoder&#34;))
  self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
  self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

  d = {
    &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight,
    &#34;prior_batch_size&#34;: self.prior_batch_size,
    &#34;code_regularisation_weight&#34;: self.code_regularisation_weight
  }

  # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
  #   json.dump(d,f)

  (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.scramble_font_batches"><code class="name flex">
<span>def <span class="ident">scramble_font_batches</span></span>(<span>self, x: tensorflow.python.framework.ops.Tensor, labels: tensorflow.python.framework.ops.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>Feature tensor</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>Label tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]</code></dt>
<dd>return scrambled and original feature-label pairs, in that order.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scramble_font_batches(self, x: tf.Tensor, labels: tf.Tensor):
  &#34;&#34;&#34;Creates a scrambled copy of a minibatch in which individual fonts are randomly shuffled. Returns the original minibatch in addition to the shuffled version.
  
  Args:
      x (tf.Tensor): Feature tensor
      labels (tf.Tensor): Label tensor
  
  Returns:
      t.Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: return scrambled and original feature-label pairs, in that order.
  &#34;&#34;&#34;
  #
  x_shape = tf.shape(x)
  n_fonts = x_shape[0]
  #
  style_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  style_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)

  outcome_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  outcome_y = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
  #
  for k in range(n_fonts):
    x_k, labels_k = x[k], labels[k]
    x_k_shape = tf.shape(x_k)
    shuffling_idx = tf.range(start=0, limit=tf.shape(x_k)[0], dtype=tf.int32)
    scrambled = tf.random.shuffle(shuffling_idx)
    #
    style_x = style_x.write(k, tf.gather(x_k, scrambled, axis=0))
    style_y = style_y.write(k, tf.gather(labels_k, scrambled, axis=0))

    outcome_x = outcome_x.write(k, x_k)
    outcome_y = outcome_y.write(k, labels_k)
    #
  #
  #
  return style_x.concat(), style_y.concat(), outcome_x.concat(), outcome_y.concat()</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.PureFontStyleSAAE.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <code>Model.make_train_function</code>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <code>tf.function</code> and
<code>tf.distribute.Strategy</code> settings), should be left to
<code>Model.make_train_function</code>, which can also be overridden.</p>
<h2 id="arguments">Arguments</h2>
<p>data: A nested structure of <code>Tensor</code>s.</p>
<h2 id="returns">Returns</h2>
<p>A <code>dict</code> containing values that will be passed to
<code>tf.keras.callbacks.CallbackList.on_train_batch_end</code>. Typically, the
values of the <code>Model</code>'s metrics are returned. Example:
<code>{'loss': 0.2, 'accuracy': 0.7}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(self, inputs):
  #prior_sampler = tf.random.normal
  x, labels = inputs

  #self.prior_batch_size = x.shape[0]
  style_x, style_y, outcome_x, outcome_y = self.scramble_font_batches(x ,labels)

  n_examples = tf.shape(style_x)[0]
  with tf.GradientTape(persistent=True) as tape:

    # apply autoencoder
    image_precode = self.image_encoder(style_x, training=True)
    full_precode = tf.concat([image_precode, style_y], axis=-1)
    code = self.full_encoder(full_precode, training=True)
    extended_code = tf.concat([code,outcome_y],axis=-1)
    decoded = self.decoder(extended_code,training=True)  

    # apply prior_discriminator model
    prior_samples = self.prior_sampler(shape=(n_examples,code.shape[1]))
    real = self.prior_discriminator(prior_samples,training=True)
    fake = self.prior_discriminator(code,training=True)


    # Moment regularization for embedded representation to keep it closer to standard normal
    reg = self.code_regularisation_weight*(tf.reduce_mean(code)**2 + (tf.reduce_mean(code**2) - 1.0)**2)/2

    # compute losses for the models
    reconstruction_loss = tf.keras.losses.MSE(outcome_x,decoded) 
    classification_loss = self.prior_discriminator_loss(real,fake)

    mixed_loss = -(1-self.rec_loss_weight)*(classification_loss) + self.rec_loss_weight*(reconstruction_loss + reg)


  # Compute gradients
  discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
  decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
  image_encoder_gradients = tape.gradient(mixed_loss, self.image_encoder.trainable_variables)
  full_encoder_gradients = tape.gradient(mixed_loss, self.full_encoder.trainable_variables)

  #apply gradients
  self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
  self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
  self.image_encoder.optimizer.apply_gradients(zip(image_encoder_gradients,self.image_encoder.trainable_variables))
  self.full_encoder.optimizer.apply_gradients(zip(full_encoder_gradients,self.full_encoder.trainable_variables))

  # compute metrics
  self.mse_metric.update_state(outcome_x,decoded)

  discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
  discr_predicted = tf.concat([real,fake],axis=0)
  self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  return {
  &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
  &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE"><code class="flex name class">
<span>class <span class="ident">TensorFontStyleSAAE</span></span>
<span>(</span><span>encoder: tensorflow.python.keras.engine.training.Model, decoder: tensorflow.python.keras.engine.training.Model, prior_discriminator: tensorflow.python.keras.engine.training.Model, reconstruction_loss_weight: float = 0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>This model treats characters in a font as channels in an image. The encoder takes the whole font as an image with (n_char) channels.</p>
<p>Summary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Decoder model that maps style and characters to images</dd>
<dt><strong><code>prior_discriminator</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Discriminator between the embeddings' distribution and the target distribution, e.g. multivariate standard normal.</dd>
<dt><strong><code>full_encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder that takes high-level image features and labels to produce embedded representations</dd>
<dt><strong><code>encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Encoder for image features</dd>
<dt><strong><code>reconstruction_loss_weight</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Weight of reconstruction loss at training time. Should be between 0 and 1.</dd>
<dt><strong><code>n_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>number of labeled classes</dd>
<dt><strong><code>prior_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size from prior distribution at training time</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorFontStyleSAAE(tf.keras.Model):

  &#34;&#34;&#34;This model treats characters in a font as channels in an image. The encoder takes the whole font as an image with (n_char) channels.
  &#34;&#34;&#34;
  prior_accuracy_metric = tf.keras.metrics.Accuracy(name=&#34;prior adversarial accuracy&#34;)
  mse_metric = tf.keras.metrics.MeanSquaredError(name=&#34;Reconstruction error&#34;)
  cross_entropy_metric = tf.keras.metrics.BinaryCrossentropy(name=&#34;Prior adversarial cross entropy&#34;, from_logits=False)

  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) #assumes prior_discriminator outputs 

    ## this __init__ has to be copy pasted from the model above, because Tensorflow hates good coding practices aparently
  def __init__(
    self,
    encoder: tf.keras.Model,
    decoder: tf.keras.Model,
    prior_discriminator: tf.keras.Model,
    reconstruction_loss_weight:float=0.5):
    &#34;&#34;&#34;Summary
    
    Args:
        decoder (tf.keras.Model): Decoder model that maps style and characters to images
        prior_discriminator (tf.keras.Model): Discriminator between the embeddings&#39; distribution and the target distribution, e.g. multivariate standard normal.
        full_encoder (tf.keras.Model): Encoder that takes high-level image features and labels to produce embedded representations
        encoder (tf.keras.Model): Encoder for image features
        reconstruction_loss_weight (float, optional): Weight of reconstruction loss at training time. Should be between 0 and 1.
        n_classes (int): number of labeled classes
        prior_batch_size (int): Batch size from prior distribution at training time
    &#34;&#34;&#34;
    super(TensorFontStyleSAAE, self).__init__()

    self.encoder = encoder
    self.decoder = decoder
    self.prior_discriminator = prior_discriminator
    self.rec_loss_weight = min(max(reconstruction_loss_weight,0),1)

    self.prior_sampler = tf.random.normal
    # list of embedded models as instance attributes 
    self.model_list = [&#34;encoder&#34;, &#34;decoder&#34;, &#34;prior_discriminator&#34;]


  def prior_discriminator_loss(self,real,fake, n_fonts):
    real_loss = self.cross_entropy(tf.ones_like(real), real)
    fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
    return (real_loss + fake_loss)/tf.cast(n_fonts, tf.float32)

  def compile(self,
    optimizer=&#39;rmsprop&#39;,
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    **kwargs):

    self.encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
    self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

    super().compile(
      optimizer=optimizer,
      loss=loss,
      metrics=metrics,
      loss_weights=loss_weights,
      weighted_metrics=weighted_metrics,
      run_eagerly=run_eagerly,
      **kwargs)

  def __call__(self, x, training=True, mask=None):
    return self.encoder(x, training=training)

  def train_step(self, inputs):
    #prior_sampler = tf.random.normal
    x, _ = inputs

    x_shape = tf.shape(x)
    n_fonts = x_shape[0]
    n_chars = x_shape[1]
    height = x_shape[2]
    width = x_shape[3]

    x = tf.reshape(x, (n_fonts, n_chars, height, width))
    x = tf.transpose(x, [0,2,3,1])

    with tf.GradientTape(persistent=True) as tape:

      # apply autoencoder
      embedding = self.encoder(x, training=True)
      decoded = self.decoder(embedding,training=True)  

      # apply prior_discriminator model
      prior_samples = self.prior_sampler(shape=(n_fonts,embedding.shape[1]))
      real = self.prior_discriminator(prior_samples,training=True)
      fake = self.prior_discriminator(embedding,training=True)

      # compute losses for the models
      reconstruction_loss = tf.keras.losses.MSE(x,decoded) 
      classification_loss = self.prior_discriminator_loss(real,fake, n_fonts)

      mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

    # Compute gradients
    discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
    decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
    encoder_gradients = tape.gradient(mixed_loss, self.encoder.trainable_variables)

    #apply gradients
    self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
    self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
    self.encoder.optimizer.apply_gradients(zip(encoder_gradients,self.encoder.trainable_variables))

    # compute metrics
    self.mse_metric.update_state(x,decoded)

    discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
    discr_predicted = tf.concat([real,fake],axis=0)
    self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

    return {
    &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
    &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}



  def save(self,output_dir: str):
    &#34;&#34;&#34;Save the model to an output folder
    
    Args:
        output_dir (str): Target output folder
    &#34;&#34;&#34;

    self.encoder.save(str(BytestreamPath(output_dir) / &#34;encoder&#34;))
    self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
    self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

    d = {
      &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight
      }

    # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
    #   json.dump(d,f)

    (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())

  @classmethod
  def load(cls, input_dir: str):
    &#34;&#34;&#34;Loads a saved instance of this class
    
    Args:
        input_dir (str): Target input folder
    
    Returns:
        SAAE: Loaded model
    &#34;&#34;&#34;
    encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;encoder&#34;))
    decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
    prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

    # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
    #   d = json.loads(f.read())
    d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
    d = json.loads(d_string)

    return cls(
      encoder = encoder, 
      decoder = decoder, 
      prior_discriminator = prior_discriminator, 
      **d)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.network.Network</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.cross_entropy"><code class="name">var <span class="ident">cross_entropy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.cross_entropy_metric"><code class="name">var <span class="ident">cross_entropy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.mse_metric"><code class="name">var <span class="ident">mse_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.prior_accuracy_metric"><code class="name">var <span class="ident">prior_accuracy_metric</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>input_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a saved instance of this class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target input folder</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SAAE</code></dt>
<dd>Loaded model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, input_dir: str):
  &#34;&#34;&#34;Loads a saved instance of this class
  
  Args:
      input_dir (str): Target input folder
  
  Returns:
      SAAE: Loaded model
  &#34;&#34;&#34;
  encoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;encoder&#34;))
  decoder = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;decoder&#34;))
  prior_discriminator = tf.keras.models.load_model(str(BytestreamPath(input_dir) / &#34;prior_discriminator&#34;))

  # with open(str(BytestreamPath(input_dir) / &#34;aae-params.json&#34;),&#34;r&#34;) as f:
  #   d = json.loads(f.read())
  d_string = (BytestreamPath(input_dir) / &#34;aae-params.json&#34;).read_bytes().decode(&#34;utf-8&#34;)
  d = json.loads(d_string)

  return cls(
    encoder = encoder, 
    decoder = decoder, 
    prior_discriminator = prior_discriminator, 
    **d)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the model for training.</p>
<h2 id="arguments">Arguments</h2>
<p>optimizer: String (name of optimizer) or optimizer instance.
See <code>tf.keras.optimizers</code>.
loss: String (name of objective function), objective function or
<code>tf.keras.losses.Loss</code> instance. See <code>tf.keras.losses</code>.
An objective function is any callable with the signature
<code>loss = fn(y_true, y_pred)</code>, where
y_true = ground truth values with shape = <code>[batch_size, d0, .. dN]</code>,
except sparse loss functions such as sparse categorical crossentropy
where shape = <code>[batch_size, d0, .. dN-1]</code>.
y_pred = predicted values with shape = <code>[batch_size, d0, .. dN]</code>.
It returns a weighted loss float tensor.
If a custom <code>Loss</code> instance is used and reduction is set to NONE,
return value has the shape [batch_size, d0, .. dN-1] ie. per-sample
or per-timestep loss values; otherwise, it is a scalar.
If the model has multiple outputs, you can use a different loss on
each output by passing a dictionary or a list of losses. The loss
value that will be minimized by the model will then be the sum of
all individual losses.
metrics: List of metrics to be evaluated by the model during training
and testing.
Each of this can be a string (name of a built-in function), function
or a <code>tf.keras.metrics.Metric</code> instance. See <code>tf.keras.metrics</code>.
Typically you will use <code>metrics=['accuracy']</code>. A function is any
callable with the signature <code>result = fn(y_true, y_pred)</code>.
To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<code>metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}</code>.
You can also pass a list (len = len(outputs)) of lists of metrics
such as <code>metrics=[['accuracy'], ['accuracy', 'mse']]</code> or
<code>metrics=['accuracy', ['accuracy', 'mse']]</code>.
When you pass the strings 'accuracy' or 'acc', we convert this to
one of <code>tf.keras.metrics.BinaryAccuracy</code>,
<code>tf.keras.metrics.CategoricalAccuracy</code>,
<code>tf.keras.metrics.SparseCategoricalAccuracy</code> based on the loss
function used and the model output shape. We do a similar conversion
for the strings 'crossentropy' and 'ce' as well.
loss_weights: Optional list or dictionary specifying scalar
coefficients (Python floats) to weight the loss contributions
of different model outputs.
The loss value that will be minimized by the model
will then be the <em>weighted sum</em> of all individual losses,
weighted by the <code>loss_weights</code> coefficients.
If a list, it is expected to have a 1:1 mapping
to the model's outputs. If a dict, it is expected to map
output names (strings) to scalar coefficients.
sample_weight_mode: If you need to do timestep-wise
sample weighting (2D weights), set this to <code>"temporal"</code>.
<code>None</code> defaults to sample-wise weights (1D).
If the model has multiple outputs, you can use a different
<code>sample_weight_mode</code> on each output by passing a
dictionary or a list of modes.
weighted_metrics: List of metrics to be evaluated and weighted
by sample_weight or class_weight during training and testing.
**kwargs: Any additional arguments. For eager execution, pass
<code>run_eagerly=True</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case of invalid arguments for
<code>optimizer</code>, <code>loss</code>, <code>metrics</code> or <code>sample_weight_mode</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self,
  optimizer=&#39;rmsprop&#39;,
  loss=None,
  metrics=None,
  loss_weights=None,
  weighted_metrics=None,
  run_eagerly=None,
  **kwargs):

  self.encoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.decoder.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)
  self.prior_discriminator.compile(optimizer = copy.deepcopy(optimizer),run_eagerly=run_eagerly)  

  super().compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics,
    loss_weights=loss_weights,
    weighted_metrics=weighted_metrics,
    run_eagerly=run_eagerly,
    **kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.prior_discriminator_loss"><code class="name flex">
<span>def <span class="ident">prior_discriminator_loss</span></span>(<span>self, real, fake, n_fonts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prior_discriminator_loss(self,real,fake, n_fonts):
  real_loss = self.cross_entropy(tf.ones_like(real), real)
  fake_loss = self.cross_entropy(tf.zeros_like(fake), fake)
  return (real_loss + fake_loss)/tf.cast(n_fonts, tf.float32)</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, output_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to an output folder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Target output folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self,output_dir: str):
  &#34;&#34;&#34;Save the model to an output folder
  
  Args:
      output_dir (str): Target output folder
  &#34;&#34;&#34;

  self.encoder.save(str(BytestreamPath(output_dir) / &#34;encoder&#34;))
  self.decoder.save(str(BytestreamPath(output_dir) / &#34;decoder&#34;))
  self.prior_discriminator.save(str(BytestreamPath(output_dir) / &#34;prior_discriminator&#34;))

  d = {
    &#34;reconstruction_loss_weight&#34;:self.rec_loss_weight
    }

  # with open(str(BytestreamPath(output_dir) / &#34;aae-params.json&#34;),&#34;w&#34;) as f:
  #   json.dump(d,f)

  (BytestreamPath(output_dir) / &#34;aae-params.json&#34;).write_bytes(json.dumps(d).encode())</code></pre>
</details>
</dd>
<dt id="fontai.prediction.models.TensorFontStyleSAAE.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <code>Model.make_train_function</code>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <code>tf.function</code> and
<code>tf.distribute.Strategy</code> settings), should be left to
<code>Model.make_train_function</code>, which can also be overridden.</p>
<h2 id="arguments">Arguments</h2>
<p>data: A nested structure of <code>Tensor</code>s.</p>
<h2 id="returns">Returns</h2>
<p>A <code>dict</code> containing values that will be passed to
<code>tf.keras.callbacks.CallbackList.on_train_batch_end</code>. Typically, the
values of the <code>Model</code>'s metrics are returned. Example:
<code>{'loss': 0.2, 'accuracy': 0.7}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(self, inputs):
  #prior_sampler = tf.random.normal
  x, _ = inputs

  x_shape = tf.shape(x)
  n_fonts = x_shape[0]
  n_chars = x_shape[1]
  height = x_shape[2]
  width = x_shape[3]

  x = tf.reshape(x, (n_fonts, n_chars, height, width))
  x = tf.transpose(x, [0,2,3,1])

  with tf.GradientTape(persistent=True) as tape:

    # apply autoencoder
    embedding = self.encoder(x, training=True)
    decoded = self.decoder(embedding,training=True)  

    # apply prior_discriminator model
    prior_samples = self.prior_sampler(shape=(n_fonts,embedding.shape[1]))
    real = self.prior_discriminator(prior_samples,training=True)
    fake = self.prior_discriminator(embedding,training=True)

    # compute losses for the models
    reconstruction_loss = tf.keras.losses.MSE(x,decoded) 
    classification_loss = self.prior_discriminator_loss(real,fake, n_fonts)

    mixed_loss = -(1-self.rec_loss_weight)*classification_loss + self.rec_loss_weight*reconstruction_loss

  # Compute gradients
  discr_gradients = tape.gradient(classification_loss,self.prior_discriminator.trainable_variables)
  decoder_gradients = tape.gradient(reconstruction_loss, self.decoder.trainable_variables)
  encoder_gradients = tape.gradient(mixed_loss, self.encoder.trainable_variables)

  #apply gradients
  self.prior_discriminator.optimizer.apply_gradients(zip(discr_gradients,self.prior_discriminator.trainable_variables))
  self.decoder.optimizer.apply_gradients(zip(decoder_gradients,self.decoder.trainable_variables))
  self.encoder.optimizer.apply_gradients(zip(encoder_gradients,self.encoder.trainable_variables))

  # compute metrics
  self.mse_metric.update_state(x,decoded)

  discr_true = tf.concat([tf.ones_like(real),tf.zeros_like(fake)],axis=0)
  discr_predicted = tf.concat([real,fake],axis=0)
  self.prior_accuracy_metric.update_state(discr_true,tf.round(discr_predicted))

  return {
  &#34;reconstruction MSE&#34;: self.mse_metric.result(), 
  &#34;discriminator accuracy&#34;: self.prior_accuracy_metric.result()}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fontai.prediction" href="index.html">fontai.prediction</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fontai.prediction.models.CharStyleSAAE" href="#fontai.prediction.models.CharStyleSAAE">CharStyleSAAE</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.models.CharStyleSAAE.compile" href="#fontai.prediction.models.CharStyleSAAE.compile">compile</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.cross_entropy" href="#fontai.prediction.models.CharStyleSAAE.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.cross_entropy_metric" href="#fontai.prediction.models.CharStyleSAAE.cross_entropy_metric">cross_entropy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.load" href="#fontai.prediction.models.CharStyleSAAE.load">load</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.metrics" href="#fontai.prediction.models.CharStyleSAAE.metrics">metrics</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.mse_metric" href="#fontai.prediction.models.CharStyleSAAE.mse_metric">mse_metric</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.predict" href="#fontai.prediction.models.CharStyleSAAE.predict">predict</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.prior_accuracy_metric" href="#fontai.prediction.models.CharStyleSAAE.prior_accuracy_metric">prior_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.prior_discriminator_loss" href="#fontai.prediction.models.CharStyleSAAE.prior_discriminator_loss">prior_discriminator_loss</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.save" href="#fontai.prediction.models.CharStyleSAAE.save">save</a></code></li>
<li><code><a title="fontai.prediction.models.CharStyleSAAE.train_step" href="#fontai.prediction.models.CharStyleSAAE.train_step">train_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.models.PureCharStyleSAAE" href="#fontai.prediction.models.PureCharStyleSAAE">PureCharStyleSAAE</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.char_accuracy_metric" href="#fontai.prediction.models.PureCharStyleSAAE.char_accuracy_metric">char_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.compile" href="#fontai.prediction.models.PureCharStyleSAAE.compile">compile</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.cross_entropy" href="#fontai.prediction.models.PureCharStyleSAAE.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.cross_entropy_metric" href="#fontai.prediction.models.PureCharStyleSAAE.cross_entropy_metric">cross_entropy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.load" href="#fontai.prediction.models.PureCharStyleSAAE.load">load</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.mean_metric" href="#fontai.prediction.models.PureCharStyleSAAE.mean_metric">mean_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.mse_metric" href="#fontai.prediction.models.PureCharStyleSAAE.mse_metric">mse_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.prior_accuracy_metric" href="#fontai.prediction.models.PureCharStyleSAAE.prior_accuracy_metric">prior_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.prior_discriminator_loss" href="#fontai.prediction.models.PureCharStyleSAAE.prior_discriminator_loss">prior_discriminator_loss</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.save" href="#fontai.prediction.models.PureCharStyleSAAE.save">save</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.style_loss" href="#fontai.prediction.models.PureCharStyleSAAE.style_loss">style_loss</a></code></li>
<li><code><a title="fontai.prediction.models.PureCharStyleSAAE.train_step" href="#fontai.prediction.models.PureCharStyleSAAE.train_step">train_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.models.PureFontStyleSA2AE" href="#fontai.prediction.models.PureFontStyleSA2AE">PureFontStyleSA2AE</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.compile" href="#fontai.prediction.models.PureFontStyleSA2AE.compile">compile</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.cross_entropy" href="#fontai.prediction.models.PureFontStyleSA2AE.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.cross_entropy_metric" href="#fontai.prediction.models.PureFontStyleSA2AE.cross_entropy_metric">cross_entropy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.discriminator_loss" href="#fontai.prediction.models.PureFontStyleSA2AE.discriminator_loss">discriminator_loss</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.image_accuracy_metric" href="#fontai.prediction.models.PureFontStyleSA2AE.image_accuracy_metric">image_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.load" href="#fontai.prediction.models.PureFontStyleSA2AE.load">load</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.prior_accuracy_metric" href="#fontai.prediction.models.PureFontStyleSA2AE.prior_accuracy_metric">prior_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.save" href="#fontai.prediction.models.PureFontStyleSA2AE.save">save</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.scramble_font_batches" href="#fontai.prediction.models.PureFontStyleSA2AE.scramble_font_batches">scramble_font_batches</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSA2AE.train_step" href="#fontai.prediction.models.PureFontStyleSA2AE.train_step">train_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.models.PureFontStyleSAAE" href="#fontai.prediction.models.PureFontStyleSAAE">PureFontStyleSAAE</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.compile" href="#fontai.prediction.models.PureFontStyleSAAE.compile">compile</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.cross_entropy" href="#fontai.prediction.models.PureFontStyleSAAE.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.cross_entropy_metric" href="#fontai.prediction.models.PureFontStyleSAAE.cross_entropy_metric">cross_entropy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.load" href="#fontai.prediction.models.PureFontStyleSAAE.load">load</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.mse_metric" href="#fontai.prediction.models.PureFontStyleSAAE.mse_metric">mse_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.prior_accuracy_metric" href="#fontai.prediction.models.PureFontStyleSAAE.prior_accuracy_metric">prior_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.prior_discriminator_loss" href="#fontai.prediction.models.PureFontStyleSAAE.prior_discriminator_loss">prior_discriminator_loss</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.save" href="#fontai.prediction.models.PureFontStyleSAAE.save">save</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.scramble_font_batches" href="#fontai.prediction.models.PureFontStyleSAAE.scramble_font_batches">scramble_font_batches</a></code></li>
<li><code><a title="fontai.prediction.models.PureFontStyleSAAE.train_step" href="#fontai.prediction.models.PureFontStyleSAAE.train_step">train_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.prediction.models.TensorFontStyleSAAE" href="#fontai.prediction.models.TensorFontStyleSAAE">TensorFontStyleSAAE</a></code></h4>
<ul class="">
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.compile" href="#fontai.prediction.models.TensorFontStyleSAAE.compile">compile</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.cross_entropy" href="#fontai.prediction.models.TensorFontStyleSAAE.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.cross_entropy_metric" href="#fontai.prediction.models.TensorFontStyleSAAE.cross_entropy_metric">cross_entropy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.load" href="#fontai.prediction.models.TensorFontStyleSAAE.load">load</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.mse_metric" href="#fontai.prediction.models.TensorFontStyleSAAE.mse_metric">mse_metric</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.prior_accuracy_metric" href="#fontai.prediction.models.TensorFontStyleSAAE.prior_accuracy_metric">prior_accuracy_metric</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.prior_discriminator_loss" href="#fontai.prediction.models.TensorFontStyleSAAE.prior_discriminator_loss">prior_discriminator_loss</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.save" href="#fontai.prediction.models.TensorFontStyleSAAE.save">save</a></code></li>
<li><code><a title="fontai.prediction.models.TensorFontStyleSAAE.train_step" href="#fontai.prediction.models.TensorFontStyleSAAE.train_step">train_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>