<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fontai.runners.stages API documentation</title>
<meta name="description" content="This module contains the definitions of high-level ML lifecycle stage classes; at the moment this includes ingestion, preprocessing and training/scoring." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fontai.runners.stages</code></h1>
</header>
<section id="section-intro">
<p>This module contains the definitions of high-level ML lifecycle stage classes; at the moment this includes ingestion, preprocessing and training/scoring.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
  This module contains the definitions of high-level ML lifecycle stage classes; at the moment this includes ingestion, preprocessing and training/scoring.
&#34;&#34;&#34;

import logging

from collections import OrderedDict
from pathlib import Path
import typing
import traceback
import os
import string
import sys
import signal
import typing as t

from PIL import ImageFont
from numpy import ndarray
from strictyaml import as_document

from tensorflow import Tensor
import tensorflow as tf
from tensorflow.data import TFRecordDataset

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import SetupOptions

from fontai.config.preprocessing import Config as ProcessingConfig, ConfigHandler as ProcessingConfigHandler
from fontai.config.ingestion import Config as IngestionConfig, ConfigHandler as IngestionConfigHandler
from fontai.config.prediction import ModelFactory, TrainingConfig, Config as ScoringConfig, ConfigHandler as ScoringConfigHandler
from fontai.config.deployment import Config as DeploymentConfig, ConfigHandler as DeploymentConfigHandler, Grid

from fontai.prediction.input_processing import RecordPreprocessor
from fontai.preprocessing.mappings import PipelineFactory, BeamCompatibleWrapper, Writer, ZipToFontFiles

#from fontai.runners.base import MLPipelineTransform
from fontai.io.storage import BytestreamPath
from fontai.io.readers import ScrapperReader, ZipReader, TfrReader
from fontai.io.writers import TfrWriter, ZipWriter
from fontai.io.records import TfrWritable
from fontai.io.formats import InMemoryFile, InMemoryZipfile
from fontai.runners.base import ConfigurableTransform, IdentityTransform, FittableTransform

from fontai.deployment.plotters import AlphabetPlotter

import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output

from numpy import array as np_array, clip as np_clip

import mlflow 
  
__all__ = [
  &#34;Ingestion&#34;,
  &#34;Preprocessing&#34;,
  &#34;Scoring&#34;,
  &#34;Deployment&#34;
  ]

logger = logging.getLogger(__name__)
  
class Ingestion(ConfigurableTransform, IdentityTransform):

  &#34;&#34;&#34;Retrieves zipped font files. It takes a list of scrappers defined in `fontai.io.scrappers` from which it downloads files to storage.
  
  &#34;&#34;&#34;

  def __init__(self, config: IngestionConfig = None):
    
    self.config = config

  @classmethod
  def get_config_parser(cls):
    return IngestionConfigHandler()

  @classmethod
  def from_config_object(cls, config: IngestionConfig, **kwargs):
    return cls(config)

  @classmethod
  def run_from_config_object(cls, config: IngestionConfig, **kwargs):
    
    ingestor = cls.from_config_object(config)
    font_extractor = ZipToFontFiles()
    writer = ZipWriter(ingestor.config.output_path, config.max_output_file_size)
    for file in ScrapperReader(config.scrappers).get_files():
      for font_file in font_extractor.map(file):
        writer.write(font_file)

  @classmethod
  def get_stage_name(cls):
    return &#34;ingestion&#34;

  def transform_batch(self, input_path: str, output_path: str):
    raise NotImplementedError(&#34;This method is not implemented for ingestion.&#34;)


class Preprocessing(ConfigurableTransform):
  &#34;&#34;&#34;
  Processes zipped font files and outputs Tensorflow records consisting of labeled images for ML consumption.

  &#34;&#34;&#34;

  def __init__(
    self, 
    output_record_class: type,
    charset: str,
    font_extraction_size: int,
    canvas_size: int,
    canvas_padding: int,
    output_array_size: int,
    beam_cmd_line_args: t.List[str] = []):
    &#34;&#34;&#34;
    
    Args:
        output_record_class (type): Output schema class, inheriting from TfrWritable
        charset (str): String with characters to be extracted
        font_extraction_size (int): Font size to use when extracting font images
        canvas_size (int): Image canvas size in which fonts will be extracted
        canvas_padding (int): Padding in the image extraction canvas
        output_array_size (int): Final character image size
        beam_cmd_line_args (t.List[str], optional): List of Apache Beam command line arguments for distributed processing
    &#34;&#34;&#34;
    self.beam_cmd_line_args = beam_cmd_line_args

    self.pipeline = PipelineFactory.create(
      output_record_class = output_record_class,
      charset = charset,
      font_extraction_size = font_extraction_size,
      canvas_size = canvas_size,
      canvas_padding = canvas_padding,
      output_array_size = output_array_size)


  @classmethod
  def from_config_object(cls, config: ProcessingConfig, **kwargs):

    return cls(
      output_record_class = config.output_record_class,
      output_array_size = config.output_array_size,
      beam_cmd_line_args = config.beam_cmd_line_args,
      **config.font_to_array_config.dict())

    return cls()

  def transform(self, data):
    return self.pipeline.map(data)

  @classmethod
  def get_config_parser(cls):
    return ProcessingConfigHandler()

  @classmethod
  def get_stage_name(cls):
    return &#34;preprocessing&#34;

  @classmethod
  def run_from_config_object(cls, config: ProcessingConfig, **kwargs):

    # set provisional value for CUDA env variable to prevent out of memory errors from the GPU; this occurs because the preprocessing code depends on (CPU-bound) Tensorflow functionality, which attempts to seize memory from the GPU automatically, but this throws an error when Beam uses multiple threads.
    visible_devices = os.getenv(&#34;CUDA_VISIBLE_DEVICES&#34;)
    os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;-1&#34;

    output_path, input_path = config.output_path, config.input_path

    processor = cls.from_config_object(config)

    # if output is locally persisted, create parent folders
    if not BytestreamPath(output_path).is_url():
      Path(str(output_path)).mkdir(parents=True, exist_ok=True)

    pipeline_options = PipelineOptions(processor.beam_cmd_line_args)
    pipeline_options.view_as(SetupOptions).save_main_session = True

    with beam.Pipeline(options=pipeline_options) as p:

      # execute pipeline
      (p
      | &#39;create source list&#39; &gt;&gt; beam.Create(BytestreamPath(input_path).list_sources()) #create list of sources as strings
      | &#39;read zipped files&#39; &gt;&gt; beam.Map(lambda filepath: InMemoryZipfile.from_bytestream_path(filepath)) #line needed to load files lazily and not overwhelm memory
      | &#39;get labeled exampes from zip files&#39; &gt;&gt; beam.ParDo(
        BeamCompatibleWrapper(
          mapper = PipelineFactory.create(
            output_record_class = config.output_record_class,
            output_array_size = config.output_array_size,
            **config.font_to_array_config.dict())
        )
      )
      | &#34;write to storage&#34; &gt;&gt; beam.ParDo(Writer(TfrWriter(output_path, config.max_output_file_size))))

    # unset provisional value for env variable
    if visible_devices is None:
      del os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;]
    else:
      os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = visible_devices



class Scoring(FittableTransform):
  &#34;&#34;&#34;
  Trains a prediction model or uses one to score input data.
  

  Attributes:
      model (keras.Model): Scoring model
      CHARSET_OPTIONS (t.Dict): Dictionary from allowed charsets names to charsets
      training_config (TrainingConfig): training configuration wrapper
      charset_tensor (Tensor): Tensor with an entry per character in the current charset
  
  &#34;&#34;&#34;
  CHARSET_OPTIONS = {
    &#34;uppercase&#34;: string.ascii_letters[26::],
    &#34;lowercase&#34;: string.ascii_letters[0:26],
    &#34;digits&#34;: string.digits,
    &#34;all&#34;: string.ascii_letters + string.digits
    }

  def __init__(
    self, 
    model: tf.keras.Model, 
    training_config: TrainingConfig = None, 
    charset: str = &#34;lowercase&#34;):
    &#34;&#34;&#34;
    
    Args:
        model (tf.keras.Model): Scoring model
        training_config (TrainingConfig, optional): Training configuration wrapper
        charset (str): charset to use for training and batch scoring. It must be one of &#39;lowercase&#39;, &#39;uppercase&#39; or &#39;digits&#39;, or otherwise a string with all characters under consideration
    &#34;&#34;&#34;
    self.model = model
    self.training_config = training_config
    self.charset = charset


    try:
      self.charset = self.CHARSET_OPTIONS[charset]
    except KeyError as e:
      logger.warning(f&#34;Charset string is not one from {list(self.CHARSET_OPTIONS.keys())}; creating custom charset from provided string instead.&#34;)
      self.charset = &#34;&#34;.join(list(set(charset)))

    self.num_classes = len(self.charset)
    self.charset_tensor = np_array([str.encode(x) for x in list(self.charset)])


  def fit(self, data: TFRecordDataset):
    &#34;&#34;&#34;Fits the scoring model with the passed data
    
    Args:
        data (TFRecordDataset): training data
    
    Returns:
        Scoring: Scoring with trained model
    
    Raises:
        ValueError: If training_config is None (not provided).
    &#34;&#34;&#34;

    if self.training_config is None:
      raise ValueError(&#34;Training configuration not provided at instantiation time.&#34;)
    
    self.model.compile(
      loss = self.training_config.loss, 
      optimizer = self.training_config.optimizer,
      metrics = self.training_config.metrics,
      run_eagerly=False)

    self.model.fit(
      data,
      #training_data,
      steps_per_epoch = self.training_config.steps_per_epoch, 
      epochs = self.training_config.epochs,
      callbacks = self.training_config.callbacks)

    return self

  def _to_shape(self, x: t.Union[ndarray, Tensor]):
    &#34;&#34;&#34;Reshape single example to be transformed in-memory by the `transform` method.
    
    Args:
        x (t.Union[ndarray, Tensor]): Single input
    
    Returns:
        t.Union[ndarray, Tensor]: Reshaped input
    &#34;&#34;&#34;
    if len(x.shape) == 2:
      x = x.reshape((1,) + x.shape + (1,))
    elif len(x.shape) == 3:
      x = x.reshape((1,) + x.shape)
    return x

  def transform(self, input_data: t.Union[ndarray, Tensor, TfrWritable]) -&gt; t.Union[Tensor, ndarray, TfrWritable]:
    &#34;&#34;&#34;
    Process a single instance
    
    Args:
        input_data (t.Union[ndarray, Tensor, TfrWritable]): Input instance
    
    Returns:
        t.Union[Tensor, ndarray, TfrWritable]: Scored example in the corresponding format, depending on the input type.
    
    Raises:
        TypeError: If input type is not allowed.
    
    &#34;&#34;&#34;

    if isinstance(input_data, (ndarray, Tensor)):
      return self.model.predict(self._to_shape(input_data))
    elif isinstance(input_data, TfrWritable):
      return input_data.add_score(
        score = self.model.predict(self._to_shape(input_data.features)), 
        charset_tensor=self.charset_tensor)
    else:
      raise TypeError(&#34;Input type is not one of ndarray, Tensor or TfrWritable&#34;)

  @classmethod
  def get_config_parser(cls):
    return ScoringConfigHandler()

  @classmethod
  def get_stage_name(cls):
    return &#34;scoring&#34;


  @classmethod
  def from_config_object(cls, config: ScoringConfig, load_from_model_path = False):
    &#34;&#34;&#34;Initialises a Scoring instance from a configuration object
    
    Args:
        config (ScoringConfig): COnfiguration object
        load_from_model_path (bool, optional): If True, the model is loaded from the model_path argument in the configuration object.
    
    Returns:
        Scoring: Instantiated Scoring object.
    &#34;&#34;&#34;
    if load_from_model_path:
      model_class_name = config.model.__class__.__name__
      classname_tuple = (&#34;custom_class&#34;, model_class_name if model_class_name != &#34;Sequential&#34; else None)
      
      # dict -&gt; YAML -&gt; Model
      input_dict = {&#34;path&#34;: config.model_path}
      if model_class_name != &#34;Sequential&#34;:
        input_dict[&#34;custom_class&#34;] = model_class_name
      model_yaml = as_document(input_dict)

      message = f&#34;load_from_model_path flag set to True; loading model of class {model_class_name} from {config.model_path}&#34;
      #print(message)
      logger.info(message)
      model = ModelFactory().from_yaml(model_yaml)
    else:
      model = config.model

    predictor = Scoring(model = model, training_config = config.training_config, charset = config.charset)
    return predictor

  @classmethod
  def run_from_config_object(cls, config: ScoringConfig, load_from_model_path = False):
    predictor = cls.from_config_object(config, load_from_model_path)

    data_fetcher = RecordPreprocessor(
      input_record_class = config.input_record_class,
      charset_tensor = predictor.charset_tensor,
      custom_filters = predictor.training_config.custom_filters,
      custom_mappers = predictor.training_config.custom_mappers)

    writer = TfrWriter(config.output_path)

    files = TfrReader(config.input_path).get_files()
    data = data_fetcher.fetch(files, training_format=False, batch_size = predictor.training_config.batch_size)

    counter = 0
    for features, labels, fontnames in data:
      try:
        scores = predictor.transform(features)
        
        scored_records = config.input_record_class.from_scored_batch(
          features = features.numpy(),
          labels = labels.numpy(),
          fontnames = fontnames.numpy(), 
          scores = scores,
          charset_tensor = predictor.charset_tensor)

        for record in scored_records:
          writer.write(record)
        counter += 1
      except Exception as e:
        logger.exception(f&#34;Exception scoring batch with features: {features}. Full trace: {traceback.format_exc()}&#34;)
    
    writer.close()
    logger.info(f&#34;Processed {counter} examples.&#34;)

  @classmethod
  def fit_from_config_object(cls, config: ScoringConfig, load_from_model_path = False, run_id: str = None):
    
    # implement graceful interruptions
    def save_on_sigint(sig, frame):
      predictor.model.save(config.model_path)
      logger.info(f&#34;Training stopped by SIGINT: saving current model to {config.model_path}&#34;)
      sys.exit(0)
      
    signal.signal(signal.SIGINT, save_on_sigint)

    # start MLFlow run
    with mlflow.start_run(run_id=run_id, nested=False) as run:

      logger.info(f&#34;MLFlow run id: {run.info.run_id}&#34;)

      cfg_log_path = &#34;run-configs&#34;
      # check whether there are previous run configs in this MLFLow run
      client = mlflow.tracking.MlflowClient()
      n_previous_runs = len(client.list_artifacts(run.info.run_id, cfg_log_path))
      current_run = f&#34;{n_previous_runs + 1}.yaml&#34;

      # log config file
      with open(current_run,&#34;w&#34;) as f:
        f.write(config.yaml.as_yaml())
      mlflow.log_artifact(current_run,cfg_log_path)
      os.remove(current_run)

      #start keras autologging
      mlflow.tensorflow.autolog()

      # fetch data and fit model
      predictor = cls.from_config_object(config, load_from_model_path)

      data = TfrReader(config.input_path).get_files()

      data_fetcher = RecordPreprocessor(
        input_record_class = config.input_record_class,
        charset_tensor = predictor.charset_tensor,
        custom_filters = predictor.training_config.custom_filters,
        custom_mappers = predictor.training_config.custom_mappers)

      predictor.fit(data_fetcher.fetch(data, training_format=True, batch_size=predictor.training_config.batch_size))

      logger.info(f&#34;Saving trained model to {config.model_path}&#34;)
      
      predictor.model.save(config.model_path)


class Deployment(ConfigurableTransform):

  &#34;&#34;&#34;Deploys a small Dash app in which a fitted typeface generative model&#39;s style space can be explored

  &#34;&#34;&#34;
  
  def __init__(
    self,
    model: tf.keras.Model, 
    sampler: t.Callable, 
    grid: Grid, 
    plotter: AlphabetPlotter,
    charset_size: int):
    &#34;&#34;&#34;
    Args:
        model (tf.keras.Model): Generative model
        sampler (t.Callable): Style vectors&#39; sampling function
        grid (Grid): Grid describing the style space&#39;s grid to explore
        plotter (AlphabetPlotter): Plotter object
        charset_size (int): Number of characters in font
    
    &#34;&#34;&#34;
    self.model = model
    self.sampler = sampler
    self.grid = grid
    self.charset_size = charset_size
    self.plotter = plotter

    self.external_stylesheets = [&#39;https://codepen.io/chriddyp/pen/bWLwgP.css&#39;]

  def launch(self,**kwargs):
    &#34;&#34;&#34;Launches a Dash app
    
    Args:
        **kwargs: Additional arguments passed to Dash app
    &#34;&#34;&#34;
    app = dash.Dash(&#34;fontai-deployment&#34;, external_stylesheets=self.external_stylesheets)

    step = (self.grid.largest - self.grid.lowest)/self.grid.size
    middle = (self.grid.largest + self.grid.lowest)/2

    app.layout = html.Div(children=[
      html.Div(children=[html.Button(&#39;Random&#39;, id=&#39;button&#39;)] +
        [dcc.Slider(min=self.grid.lowest,max=self.grid.largest,step=step, value=middle, id=f&#34;slider-{k}&#34;) for k in range(self.grid.dim)],
        style = {&#39;display&#39;: &#39;inline-block&#39;, &#39;width&#39;: &#39;25%&#39;}),
      html.Img(id=&#34;font_figure&#34;)
      ])

    @app.callback(
      [Output(f&#34;slider-{k}&#34;,&#34;value&#34;) for k in range(self.grid.dim)],
      Input(&#34;button&#34;,&#34;n_clicks&#34;))
    def update_random(n_clicks):
      return list(np_clip(self.sampler(size=self.grid.dim), a_min=self.grid.lowest, a_max=self.grid.largest))

    @app.callback(
      Output(&#39;font_figure&#39;, &#39;src&#39;),
      [Input(f&#34;slider-{k}&#34;, &#39;value&#39;) for k in range(self.grid.dim)])
    def update(*args):
      style_vector = np_array(args).reshape((1,-1))
      font = self.plotter.generate_font(model=self.model, style_vector=style_vector, charset_size=self.charset_size)
      img = self.plotter.plot_font(font)
      return self.plotter.fig_to_str(img)

    app.run_server(**kwargs)

  def process(self, x):
    raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)

  @classmethod
  def get_config_parser(cls):
    return DeploymentConfigHandler()

  @classmethod
  def from_config_object(cls, config: DeploymentConfig, **kwargs):
    return cls(
      model = config.model,
      sampler = config.sampler,
      grid = config.grid,
      plotter = config.plotter,
      charset_size = config.charset_size)

  @classmethod
  def run_from_config_object(cls, config: DeploymentConfig, **kwargs):
    
    cls.from_config_object(config).launch(**config.dash_args)

  @classmethod
  def get_stage_name(cls):
    return &#34;deployment&#34;

  def transform_batch(self, input_path: str, output_path: str):
    raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fontai.runners.stages.Deployment"><code class="flex name class">
<span>class <span class="ident">Deployment</span></span>
<span>(</span><span>model: tensorflow.python.keras.engine.training.Model, sampler: Callable, grid: <a title="fontai.config.deployment.Grid" href="../config/deployment.html#fontai.config.deployment.Grid">Grid</a>, plotter: <a title="fontai.deployment.plotters.AlphabetPlotter" href="../deployment/plotters.html#fontai.deployment.plotters.AlphabetPlotter">AlphabetPlotter</a>, charset_size: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Deploys a small Dash app in which a fitted typeface generative model's style space can be explored</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Generative model</dd>
<dt><strong><code>sampler</code></strong> :&ensp;<code>t.Callable</code></dt>
<dd>Style vectors' sampling function</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>Grid</code></dt>
<dd>Grid describing the style space's grid to explore</dd>
<dt><strong><code>plotter</code></strong> :&ensp;<code>AlphabetPlotter</code></dt>
<dd>Plotter object</dd>
<dt><strong><code>charset_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of characters in font</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Deployment(ConfigurableTransform):

  &#34;&#34;&#34;Deploys a small Dash app in which a fitted typeface generative model&#39;s style space can be explored

  &#34;&#34;&#34;
  
  def __init__(
    self,
    model: tf.keras.Model, 
    sampler: t.Callable, 
    grid: Grid, 
    plotter: AlphabetPlotter,
    charset_size: int):
    &#34;&#34;&#34;
    Args:
        model (tf.keras.Model): Generative model
        sampler (t.Callable): Style vectors&#39; sampling function
        grid (Grid): Grid describing the style space&#39;s grid to explore
        plotter (AlphabetPlotter): Plotter object
        charset_size (int): Number of characters in font
    
    &#34;&#34;&#34;
    self.model = model
    self.sampler = sampler
    self.grid = grid
    self.charset_size = charset_size
    self.plotter = plotter

    self.external_stylesheets = [&#39;https://codepen.io/chriddyp/pen/bWLwgP.css&#39;]

  def launch(self,**kwargs):
    &#34;&#34;&#34;Launches a Dash app
    
    Args:
        **kwargs: Additional arguments passed to Dash app
    &#34;&#34;&#34;
    app = dash.Dash(&#34;fontai-deployment&#34;, external_stylesheets=self.external_stylesheets)

    step = (self.grid.largest - self.grid.lowest)/self.grid.size
    middle = (self.grid.largest + self.grid.lowest)/2

    app.layout = html.Div(children=[
      html.Div(children=[html.Button(&#39;Random&#39;, id=&#39;button&#39;)] +
        [dcc.Slider(min=self.grid.lowest,max=self.grid.largest,step=step, value=middle, id=f&#34;slider-{k}&#34;) for k in range(self.grid.dim)],
        style = {&#39;display&#39;: &#39;inline-block&#39;, &#39;width&#39;: &#39;25%&#39;}),
      html.Img(id=&#34;font_figure&#34;)
      ])

    @app.callback(
      [Output(f&#34;slider-{k}&#34;,&#34;value&#34;) for k in range(self.grid.dim)],
      Input(&#34;button&#34;,&#34;n_clicks&#34;))
    def update_random(n_clicks):
      return list(np_clip(self.sampler(size=self.grid.dim), a_min=self.grid.lowest, a_max=self.grid.largest))

    @app.callback(
      Output(&#39;font_figure&#39;, &#39;src&#39;),
      [Input(f&#34;slider-{k}&#34;, &#39;value&#39;) for k in range(self.grid.dim)])
    def update(*args):
      style_vector = np_array(args).reshape((1,-1))
      font = self.plotter.generate_font(model=self.model, style_vector=style_vector, charset_size=self.charset_size)
      img = self.plotter.plot_font(font)
      return self.plotter.fig_to_str(img)

    app.run_server(**kwargs)

  def process(self, x):
    raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)

  @classmethod
  def get_config_parser(cls):
    return DeploymentConfigHandler()

  @classmethod
  def from_config_object(cls, config: DeploymentConfig, **kwargs):
    return cls(
      model = config.model,
      sampler = config.sampler,
      grid = config.grid,
      plotter = config.plotter,
      charset_size = config.charset_size)

  @classmethod
  def run_from_config_object(cls, config: DeploymentConfig, **kwargs):
    
    cls.from_config_object(config).launch(**config.dash_args)

  @classmethod
  def get_stage_name(cls):
    return &#34;deployment&#34;

  def transform_batch(self, input_path: str, output_path: str):
    raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></li>
<li><a title="fontai.runners.base.Transform" href="base.html#fontai.runners.base.Transform">Transform</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="fontai.runners.stages.Deployment.get_stage_name"><code class="name flex">
<span>def <span class="ident">get_stage_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_stage_name(cls):
  return &#34;deployment&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.runners.stages.Deployment.launch"><code class="name flex">
<span>def <span class="ident">launch</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Launches a Dash app</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to Dash app</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def launch(self,**kwargs):
  &#34;&#34;&#34;Launches a Dash app
  
  Args:
      **kwargs: Additional arguments passed to Dash app
  &#34;&#34;&#34;
  app = dash.Dash(&#34;fontai-deployment&#34;, external_stylesheets=self.external_stylesheets)

  step = (self.grid.largest - self.grid.lowest)/self.grid.size
  middle = (self.grid.largest + self.grid.lowest)/2

  app.layout = html.Div(children=[
    html.Div(children=[html.Button(&#39;Random&#39;, id=&#39;button&#39;)] +
      [dcc.Slider(min=self.grid.lowest,max=self.grid.largest,step=step, value=middle, id=f&#34;slider-{k}&#34;) for k in range(self.grid.dim)],
      style = {&#39;display&#39;: &#39;inline-block&#39;, &#39;width&#39;: &#39;25%&#39;}),
    html.Img(id=&#34;font_figure&#34;)
    ])

  @app.callback(
    [Output(f&#34;slider-{k}&#34;,&#34;value&#34;) for k in range(self.grid.dim)],
    Input(&#34;button&#34;,&#34;n_clicks&#34;))
  def update_random(n_clicks):
    return list(np_clip(self.sampler(size=self.grid.dim), a_min=self.grid.lowest, a_max=self.grid.largest))

  @app.callback(
    Output(&#39;font_figure&#39;, &#39;src&#39;),
    [Input(f&#34;slider-{k}&#34;, &#39;value&#39;) for k in range(self.grid.dim)])
  def update(*args):
    style_vector = np_array(args).reshape((1,-1))
    font = self.plotter.generate_font(model=self.model, style_vector=style_vector, charset_size=self.charset_size)
    img = self.plotter.plot_font(font)
    return self.plotter.fig_to_str(img)

  app.run_server(**kwargs)</code></pre>
</details>
</dd>
<dt id="fontai.runners.stages.Deployment.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(self, x):
  raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)</code></pre>
</details>
</dd>
<dt id="fontai.runners.stages.Deployment.transform_batch"><code class="name flex">
<span>def <span class="ident">transform_batch</span></span>(<span>self, input_path: str, output_path: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_batch(self, input_path: str, output_path: str):
  raise NotImplementedError(&#34;This method is not implemented for deployment.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_file">from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_object">from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_str">from_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.get_config_parser" href="base.html#fontai.runners.base.ConfigurableTransform.get_config_parser">get_config_parser</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_file">parse_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_str">parse_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_file">run_from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_object">run_from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.transform" href="base.html#fontai.runners.base.Transform.transform">transform</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fontai.runners.stages.Ingestion"><code class="flex name class">
<span>class <span class="ident">Ingestion</span></span>
<span>(</span><span>config: <a title="fontai.config.ingestion.Config" href="../config/ingestion.html#fontai.config.ingestion.Config">Config</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves zipped font files. It takes a list of scrappers defined in <code><a title="fontai.io.scrappers" href="../io/scrappers.html">fontai.io.scrappers</a></code> from which it downloads files to storage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Ingestion(ConfigurableTransform, IdentityTransform):

  &#34;&#34;&#34;Retrieves zipped font files. It takes a list of scrappers defined in `fontai.io.scrappers` from which it downloads files to storage.
  
  &#34;&#34;&#34;

  def __init__(self, config: IngestionConfig = None):
    
    self.config = config

  @classmethod
  def get_config_parser(cls):
    return IngestionConfigHandler()

  @classmethod
  def from_config_object(cls, config: IngestionConfig, **kwargs):
    return cls(config)

  @classmethod
  def run_from_config_object(cls, config: IngestionConfig, **kwargs):
    
    ingestor = cls.from_config_object(config)
    font_extractor = ZipToFontFiles()
    writer = ZipWriter(ingestor.config.output_path, config.max_output_file_size)
    for file in ScrapperReader(config.scrappers).get_files():
      for font_file in font_extractor.map(file):
        writer.write(font_file)

  @classmethod
  def get_stage_name(cls):
    return &#34;ingestion&#34;

  def transform_batch(self, input_path: str, output_path: str):
    raise NotImplementedError(&#34;This method is not implemented for ingestion.&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></li>
<li><a title="fontai.runners.base.IdentityTransform" href="base.html#fontai.runners.base.IdentityTransform">IdentityTransform</a></li>
<li><a title="fontai.runners.base.Transform" href="base.html#fontai.runners.base.Transform">Transform</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="fontai.runners.stages.Ingestion.get_stage_name"><code class="name flex">
<span>def <span class="ident">get_stage_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_stage_name(cls):
  return &#34;ingestion&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.runners.stages.Ingestion.transform_batch"><code class="name flex">
<span>def <span class="ident">transform_batch</span></span>(<span>self, input_path: str, output_path: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_batch(self, input_path: str, output_path: str):
  raise NotImplementedError(&#34;This method is not implemented for ingestion.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_file">from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_object">from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_str">from_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.get_config_parser" href="base.html#fontai.runners.base.ConfigurableTransform.get_config_parser">get_config_parser</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_file">parse_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_str">parse_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_file">run_from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_object">run_from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.transform" href="base.html#fontai.runners.base.Transform.transform">transform</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fontai.runners.stages.Preprocessing"><code class="flex name class">
<span>class <span class="ident">Preprocessing</span></span>
<span>(</span><span>output_record_class: type, charset: str, font_extraction_size: int, canvas_size: int, canvas_padding: int, output_array_size: int, beam_cmd_line_args: List[str] = [])</span>
</code></dt>
<dd>
<div class="desc"><p>Processes zipped font files and outputs Tensorflow records consisting of labeled images for ML consumption.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_record_class</code></strong> :&ensp;<code>type</code></dt>
<dd>Output schema class, inheriting from TfrWritable</dd>
<dt><strong><code>charset</code></strong> :&ensp;<code>str</code></dt>
<dd>String with characters to be extracted</dd>
<dt><strong><code>font_extraction_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Font size to use when extracting font images</dd>
<dt><strong><code>canvas_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Image canvas size in which fonts will be extracted</dd>
<dt><strong><code>canvas_padding</code></strong> :&ensp;<code>int</code></dt>
<dd>Padding in the image extraction canvas</dd>
<dt><strong><code>output_array_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Final character image size</dd>
<dt><strong><code>beam_cmd_line_args</code></strong> :&ensp;<code>t.List[str]</code>, optional</dt>
<dd>List of Apache Beam command line arguments for distributed processing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Preprocessing(ConfigurableTransform):
  &#34;&#34;&#34;
  Processes zipped font files and outputs Tensorflow records consisting of labeled images for ML consumption.

  &#34;&#34;&#34;

  def __init__(
    self, 
    output_record_class: type,
    charset: str,
    font_extraction_size: int,
    canvas_size: int,
    canvas_padding: int,
    output_array_size: int,
    beam_cmd_line_args: t.List[str] = []):
    &#34;&#34;&#34;
    
    Args:
        output_record_class (type): Output schema class, inheriting from TfrWritable
        charset (str): String with characters to be extracted
        font_extraction_size (int): Font size to use when extracting font images
        canvas_size (int): Image canvas size in which fonts will be extracted
        canvas_padding (int): Padding in the image extraction canvas
        output_array_size (int): Final character image size
        beam_cmd_line_args (t.List[str], optional): List of Apache Beam command line arguments for distributed processing
    &#34;&#34;&#34;
    self.beam_cmd_line_args = beam_cmd_line_args

    self.pipeline = PipelineFactory.create(
      output_record_class = output_record_class,
      charset = charset,
      font_extraction_size = font_extraction_size,
      canvas_size = canvas_size,
      canvas_padding = canvas_padding,
      output_array_size = output_array_size)


  @classmethod
  def from_config_object(cls, config: ProcessingConfig, **kwargs):

    return cls(
      output_record_class = config.output_record_class,
      output_array_size = config.output_array_size,
      beam_cmd_line_args = config.beam_cmd_line_args,
      **config.font_to_array_config.dict())

    return cls()

  def transform(self, data):
    return self.pipeline.map(data)

  @classmethod
  def get_config_parser(cls):
    return ProcessingConfigHandler()

  @classmethod
  def get_stage_name(cls):
    return &#34;preprocessing&#34;

  @classmethod
  def run_from_config_object(cls, config: ProcessingConfig, **kwargs):

    # set provisional value for CUDA env variable to prevent out of memory errors from the GPU; this occurs because the preprocessing code depends on (CPU-bound) Tensorflow functionality, which attempts to seize memory from the GPU automatically, but this throws an error when Beam uses multiple threads.
    visible_devices = os.getenv(&#34;CUDA_VISIBLE_DEVICES&#34;)
    os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;-1&#34;

    output_path, input_path = config.output_path, config.input_path

    processor = cls.from_config_object(config)

    # if output is locally persisted, create parent folders
    if not BytestreamPath(output_path).is_url():
      Path(str(output_path)).mkdir(parents=True, exist_ok=True)

    pipeline_options = PipelineOptions(processor.beam_cmd_line_args)
    pipeline_options.view_as(SetupOptions).save_main_session = True

    with beam.Pipeline(options=pipeline_options) as p:

      # execute pipeline
      (p
      | &#39;create source list&#39; &gt;&gt; beam.Create(BytestreamPath(input_path).list_sources()) #create list of sources as strings
      | &#39;read zipped files&#39; &gt;&gt; beam.Map(lambda filepath: InMemoryZipfile.from_bytestream_path(filepath)) #line needed to load files lazily and not overwhelm memory
      | &#39;get labeled exampes from zip files&#39; &gt;&gt; beam.ParDo(
        BeamCompatibleWrapper(
          mapper = PipelineFactory.create(
            output_record_class = config.output_record_class,
            output_array_size = config.output_array_size,
            **config.font_to_array_config.dict())
        )
      )
      | &#34;write to storage&#34; &gt;&gt; beam.ParDo(Writer(TfrWriter(output_path, config.max_output_file_size))))

    # unset provisional value for env variable
    if visible_devices is None:
      del os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;]
    else:
      os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = visible_devices</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></li>
<li><a title="fontai.runners.base.Transform" href="base.html#fontai.runners.base.Transform">Transform</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="fontai.runners.stages.Preprocessing.get_stage_name"><code class="name flex">
<span>def <span class="ident">get_stage_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_stage_name(cls):
  return &#34;preprocessing&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_file">from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_object">from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.from_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_str">from_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.get_config_parser" href="base.html#fontai.runners.base.ConfigurableTransform.get_config_parser">get_config_parser</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_file">parse_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.parse_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_str">parse_config_str</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_file">run_from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.run_from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_object">run_from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.ConfigurableTransform.transform" href="base.html#fontai.runners.base.Transform.transform">transform</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fontai.runners.stages.Scoring"><code class="flex name class">
<span>class <span class="ident">Scoring</span></span>
<span>(</span><span>model: tensorflow.python.keras.engine.training.Model, training_config: <a title="fontai.config.prediction.TrainingConfig" href="../config/prediction.html#fontai.config.prediction.TrainingConfig">TrainingConfig</a> = None, charset: str = 'lowercase')</span>
</code></dt>
<dd>
<div class="desc"><p>Trains a prediction model or uses one to score input data.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>keras.Model</code></dt>
<dd>Scoring model</dd>
<dt><strong><code>CHARSET_OPTIONS</code></strong> :&ensp;<code>t.Dict</code></dt>
<dd>Dictionary from allowed charsets names to charsets</dd>
<dt><strong><code>training_config</code></strong> :&ensp;<code>TrainingConfig</code></dt>
<dd>training configuration wrapper</dd>
<dt><strong><code>charset_tensor</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Tensor with an entry per character in the current charset</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>Scoring model</dd>
<dt><strong><code>training_config</code></strong> :&ensp;<code>TrainingConfig</code>, optional</dt>
<dd>Training configuration wrapper</dd>
<dt><strong><code>charset</code></strong> :&ensp;<code>str</code></dt>
<dd>charset to use for training and batch scoring. It must be one of 'lowercase', 'uppercase' or 'digits', or otherwise a string with all characters under consideration</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Scoring(FittableTransform):
  &#34;&#34;&#34;
  Trains a prediction model or uses one to score input data.
  

  Attributes:
      model (keras.Model): Scoring model
      CHARSET_OPTIONS (t.Dict): Dictionary from allowed charsets names to charsets
      training_config (TrainingConfig): training configuration wrapper
      charset_tensor (Tensor): Tensor with an entry per character in the current charset
  
  &#34;&#34;&#34;
  CHARSET_OPTIONS = {
    &#34;uppercase&#34;: string.ascii_letters[26::],
    &#34;lowercase&#34;: string.ascii_letters[0:26],
    &#34;digits&#34;: string.digits,
    &#34;all&#34;: string.ascii_letters + string.digits
    }

  def __init__(
    self, 
    model: tf.keras.Model, 
    training_config: TrainingConfig = None, 
    charset: str = &#34;lowercase&#34;):
    &#34;&#34;&#34;
    
    Args:
        model (tf.keras.Model): Scoring model
        training_config (TrainingConfig, optional): Training configuration wrapper
        charset (str): charset to use for training and batch scoring. It must be one of &#39;lowercase&#39;, &#39;uppercase&#39; or &#39;digits&#39;, or otherwise a string with all characters under consideration
    &#34;&#34;&#34;
    self.model = model
    self.training_config = training_config
    self.charset = charset


    try:
      self.charset = self.CHARSET_OPTIONS[charset]
    except KeyError as e:
      logger.warning(f&#34;Charset string is not one from {list(self.CHARSET_OPTIONS.keys())}; creating custom charset from provided string instead.&#34;)
      self.charset = &#34;&#34;.join(list(set(charset)))

    self.num_classes = len(self.charset)
    self.charset_tensor = np_array([str.encode(x) for x in list(self.charset)])


  def fit(self, data: TFRecordDataset):
    &#34;&#34;&#34;Fits the scoring model with the passed data
    
    Args:
        data (TFRecordDataset): training data
    
    Returns:
        Scoring: Scoring with trained model
    
    Raises:
        ValueError: If training_config is None (not provided).
    &#34;&#34;&#34;

    if self.training_config is None:
      raise ValueError(&#34;Training configuration not provided at instantiation time.&#34;)
    
    self.model.compile(
      loss = self.training_config.loss, 
      optimizer = self.training_config.optimizer,
      metrics = self.training_config.metrics,
      run_eagerly=False)

    self.model.fit(
      data,
      #training_data,
      steps_per_epoch = self.training_config.steps_per_epoch, 
      epochs = self.training_config.epochs,
      callbacks = self.training_config.callbacks)

    return self

  def _to_shape(self, x: t.Union[ndarray, Tensor]):
    &#34;&#34;&#34;Reshape single example to be transformed in-memory by the `transform` method.
    
    Args:
        x (t.Union[ndarray, Tensor]): Single input
    
    Returns:
        t.Union[ndarray, Tensor]: Reshaped input
    &#34;&#34;&#34;
    if len(x.shape) == 2:
      x = x.reshape((1,) + x.shape + (1,))
    elif len(x.shape) == 3:
      x = x.reshape((1,) + x.shape)
    return x

  def transform(self, input_data: t.Union[ndarray, Tensor, TfrWritable]) -&gt; t.Union[Tensor, ndarray, TfrWritable]:
    &#34;&#34;&#34;
    Process a single instance
    
    Args:
        input_data (t.Union[ndarray, Tensor, TfrWritable]): Input instance
    
    Returns:
        t.Union[Tensor, ndarray, TfrWritable]: Scored example in the corresponding format, depending on the input type.
    
    Raises:
        TypeError: If input type is not allowed.
    
    &#34;&#34;&#34;

    if isinstance(input_data, (ndarray, Tensor)):
      return self.model.predict(self._to_shape(input_data))
    elif isinstance(input_data, TfrWritable):
      return input_data.add_score(
        score = self.model.predict(self._to_shape(input_data.features)), 
        charset_tensor=self.charset_tensor)
    else:
      raise TypeError(&#34;Input type is not one of ndarray, Tensor or TfrWritable&#34;)

  @classmethod
  def get_config_parser(cls):
    return ScoringConfigHandler()

  @classmethod
  def get_stage_name(cls):
    return &#34;scoring&#34;


  @classmethod
  def from_config_object(cls, config: ScoringConfig, load_from_model_path = False):
    &#34;&#34;&#34;Initialises a Scoring instance from a configuration object
    
    Args:
        config (ScoringConfig): COnfiguration object
        load_from_model_path (bool, optional): If True, the model is loaded from the model_path argument in the configuration object.
    
    Returns:
        Scoring: Instantiated Scoring object.
    &#34;&#34;&#34;
    if load_from_model_path:
      model_class_name = config.model.__class__.__name__
      classname_tuple = (&#34;custom_class&#34;, model_class_name if model_class_name != &#34;Sequential&#34; else None)
      
      # dict -&gt; YAML -&gt; Model
      input_dict = {&#34;path&#34;: config.model_path}
      if model_class_name != &#34;Sequential&#34;:
        input_dict[&#34;custom_class&#34;] = model_class_name
      model_yaml = as_document(input_dict)

      message = f&#34;load_from_model_path flag set to True; loading model of class {model_class_name} from {config.model_path}&#34;
      #print(message)
      logger.info(message)
      model = ModelFactory().from_yaml(model_yaml)
    else:
      model = config.model

    predictor = Scoring(model = model, training_config = config.training_config, charset = config.charset)
    return predictor

  @classmethod
  def run_from_config_object(cls, config: ScoringConfig, load_from_model_path = False):
    predictor = cls.from_config_object(config, load_from_model_path)

    data_fetcher = RecordPreprocessor(
      input_record_class = config.input_record_class,
      charset_tensor = predictor.charset_tensor,
      custom_filters = predictor.training_config.custom_filters,
      custom_mappers = predictor.training_config.custom_mappers)

    writer = TfrWriter(config.output_path)

    files = TfrReader(config.input_path).get_files()
    data = data_fetcher.fetch(files, training_format=False, batch_size = predictor.training_config.batch_size)

    counter = 0
    for features, labels, fontnames in data:
      try:
        scores = predictor.transform(features)
        
        scored_records = config.input_record_class.from_scored_batch(
          features = features.numpy(),
          labels = labels.numpy(),
          fontnames = fontnames.numpy(), 
          scores = scores,
          charset_tensor = predictor.charset_tensor)

        for record in scored_records:
          writer.write(record)
        counter += 1
      except Exception as e:
        logger.exception(f&#34;Exception scoring batch with features: {features}. Full trace: {traceback.format_exc()}&#34;)
    
    writer.close()
    logger.info(f&#34;Processed {counter} examples.&#34;)

  @classmethod
  def fit_from_config_object(cls, config: ScoringConfig, load_from_model_path = False, run_id: str = None):
    
    # implement graceful interruptions
    def save_on_sigint(sig, frame):
      predictor.model.save(config.model_path)
      logger.info(f&#34;Training stopped by SIGINT: saving current model to {config.model_path}&#34;)
      sys.exit(0)
      
    signal.signal(signal.SIGINT, save_on_sigint)

    # start MLFlow run
    with mlflow.start_run(run_id=run_id, nested=False) as run:

      logger.info(f&#34;MLFlow run id: {run.info.run_id}&#34;)

      cfg_log_path = &#34;run-configs&#34;
      # check whether there are previous run configs in this MLFLow run
      client = mlflow.tracking.MlflowClient()
      n_previous_runs = len(client.list_artifacts(run.info.run_id, cfg_log_path))
      current_run = f&#34;{n_previous_runs + 1}.yaml&#34;

      # log config file
      with open(current_run,&#34;w&#34;) as f:
        f.write(config.yaml.as_yaml())
      mlflow.log_artifact(current_run,cfg_log_path)
      os.remove(current_run)

      #start keras autologging
      mlflow.tensorflow.autolog()

      # fetch data and fit model
      predictor = cls.from_config_object(config, load_from_model_path)

      data = TfrReader(config.input_path).get_files()

      data_fetcher = RecordPreprocessor(
        input_record_class = config.input_record_class,
        charset_tensor = predictor.charset_tensor,
        custom_filters = predictor.training_config.custom_filters,
        custom_mappers = predictor.training_config.custom_mappers)

      predictor.fit(data_fetcher.fetch(data, training_format=True, batch_size=predictor.training_config.batch_size))

      logger.info(f&#34;Saving trained model to {config.model_path}&#34;)
      
      predictor.model.save(config.model_path)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fontai.runners.base.FittableTransform" href="base.html#fontai.runners.base.FittableTransform">FittableTransform</a></li>
<li><a title="fontai.runners.base.ConfigurableTransform" href="base.html#fontai.runners.base.ConfigurableTransform">ConfigurableTransform</a></li>
<li><a title="fontai.runners.base.Transform" href="base.html#fontai.runners.base.Transform">Transform</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="fontai.runners.stages.Scoring.CHARSET_OPTIONS"><code class="name">var <span class="ident">CHARSET_OPTIONS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="fontai.runners.stages.Scoring.from_config_object"><code class="name flex">
<span>def <span class="ident">from_config_object</span></span>(<span>config: <a title="fontai.config.prediction.Config" href="../config/prediction.html#fontai.config.prediction.Config">Config</a>, load_from_model_path=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialises a Scoring instance from a configuration object</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>ScoringConfig</code></dt>
<dd>COnfiguration object</dd>
<dt><strong><code>load_from_model_path</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, the model is loaded from the model_path argument in the configuration object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="fontai.runners.stages.Scoring" href="#fontai.runners.stages.Scoring">Scoring</a></code></dt>
<dd>Instantiated Scoring object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config_object(cls, config: ScoringConfig, load_from_model_path = False):
  &#34;&#34;&#34;Initialises a Scoring instance from a configuration object
  
  Args:
      config (ScoringConfig): COnfiguration object
      load_from_model_path (bool, optional): If True, the model is loaded from the model_path argument in the configuration object.
  
  Returns:
      Scoring: Instantiated Scoring object.
  &#34;&#34;&#34;
  if load_from_model_path:
    model_class_name = config.model.__class__.__name__
    classname_tuple = (&#34;custom_class&#34;, model_class_name if model_class_name != &#34;Sequential&#34; else None)
    
    # dict -&gt; YAML -&gt; Model
    input_dict = {&#34;path&#34;: config.model_path}
    if model_class_name != &#34;Sequential&#34;:
      input_dict[&#34;custom_class&#34;] = model_class_name
    model_yaml = as_document(input_dict)

    message = f&#34;load_from_model_path flag set to True; loading model of class {model_class_name} from {config.model_path}&#34;
    #print(message)
    logger.info(message)
    model = ModelFactory().from_yaml(model_yaml)
  else:
    model = config.model

  predictor = Scoring(model = model, training_config = config.training_config, charset = config.charset)
  return predictor</code></pre>
</details>
</dd>
<dt id="fontai.runners.stages.Scoring.get_stage_name"><code class="name flex">
<span>def <span class="ident">get_stage_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_stage_name(cls):
  return &#34;scoring&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fontai.runners.stages.Scoring.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data: tensorflow.python.data.ops.readers.TFRecordDatasetV2)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the scoring model with the passed data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>TFRecordDataset</code></dt>
<dd>training data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="fontai.runners.stages.Scoring" href="#fontai.runners.stages.Scoring">Scoring</a></code></dt>
<dd>Scoring with trained model</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If training_config is None (not provided).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, data: TFRecordDataset):
  &#34;&#34;&#34;Fits the scoring model with the passed data
  
  Args:
      data (TFRecordDataset): training data
  
  Returns:
      Scoring: Scoring with trained model
  
  Raises:
      ValueError: If training_config is None (not provided).
  &#34;&#34;&#34;

  if self.training_config is None:
    raise ValueError(&#34;Training configuration not provided at instantiation time.&#34;)
  
  self.model.compile(
    loss = self.training_config.loss, 
    optimizer = self.training_config.optimizer,
    metrics = self.training_config.metrics,
    run_eagerly=False)

  self.model.fit(
    data,
    #training_data,
    steps_per_epoch = self.training_config.steps_per_epoch, 
    epochs = self.training_config.epochs,
    callbacks = self.training_config.callbacks)

  return self</code></pre>
</details>
</dd>
<dt id="fontai.runners.stages.Scoring.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, input_data: Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, <a title="fontai.io.records.TfrWritable" href="../io/records.html#fontai.io.records.TfrWritable">TfrWritable</a>]) ‑> Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, <a title="fontai.io.records.TfrWritable" href="../io/records.html#fontai.io.records.TfrWritable">TfrWritable</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Process a single instance</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_data</code></strong> :&ensp;<code>t.Union[ndarray, Tensor, TfrWritable]</code></dt>
<dd>Input instance</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[Tensor, ndarray, TfrWritable]</code></dt>
<dd>Scored example in the corresponding format, depending on the input type.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If input type is not allowed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, input_data: t.Union[ndarray, Tensor, TfrWritable]) -&gt; t.Union[Tensor, ndarray, TfrWritable]:
  &#34;&#34;&#34;
  Process a single instance
  
  Args:
      input_data (t.Union[ndarray, Tensor, TfrWritable]): Input instance
  
  Returns:
      t.Union[Tensor, ndarray, TfrWritable]: Scored example in the corresponding format, depending on the input type.
  
  Raises:
      TypeError: If input type is not allowed.
  
  &#34;&#34;&#34;

  if isinstance(input_data, (ndarray, Tensor)):
    return self.model.predict(self._to_shape(input_data))
  elif isinstance(input_data, TfrWritable):
    return input_data.add_score(
      score = self.model.predict(self._to_shape(input_data.features)), 
      charset_tensor=self.charset_tensor)
  else:
    raise TypeError(&#34;Input type is not one of ndarray, Tensor or TfrWritable&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fontai.runners.base.FittableTransform" href="base.html#fontai.runners.base.FittableTransform">FittableTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="fontai.runners.base.FittableTransform.fit_from_config_file" href="base.html#fontai.runners.base.FittableTransform.fit_from_config_file">fit_from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.fit_from_config_object" href="base.html#fontai.runners.base.FittableTransform.fit_from_config_object">fit_from_config_object</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_file">from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.from_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.from_config_str">from_config_str</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.get_config_parser" href="base.html#fontai.runners.base.ConfigurableTransform.get_config_parser">get_config_parser</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.parse_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_file">parse_config_file</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.parse_config_str" href="base.html#fontai.runners.base.ConfigurableTransform.parse_config_str">parse_config_str</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.run_from_config_file" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_file">run_from_config_file</a></code></li>
<li><code><a title="fontai.runners.base.FittableTransform.run_from_config_object" href="base.html#fontai.runners.base.ConfigurableTransform.run_from_config_object">run_from_config_object</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fontai.runners" href="index.html">fontai.runners</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fontai.runners.stages.Deployment" href="#fontai.runners.stages.Deployment">Deployment</a></code></h4>
<ul class="">
<li><code><a title="fontai.runners.stages.Deployment.get_stage_name" href="#fontai.runners.stages.Deployment.get_stage_name">get_stage_name</a></code></li>
<li><code><a title="fontai.runners.stages.Deployment.launch" href="#fontai.runners.stages.Deployment.launch">launch</a></code></li>
<li><code><a title="fontai.runners.stages.Deployment.process" href="#fontai.runners.stages.Deployment.process">process</a></code></li>
<li><code><a title="fontai.runners.stages.Deployment.transform_batch" href="#fontai.runners.stages.Deployment.transform_batch">transform_batch</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.runners.stages.Ingestion" href="#fontai.runners.stages.Ingestion">Ingestion</a></code></h4>
<ul class="">
<li><code><a title="fontai.runners.stages.Ingestion.get_stage_name" href="#fontai.runners.stages.Ingestion.get_stage_name">get_stage_name</a></code></li>
<li><code><a title="fontai.runners.stages.Ingestion.transform_batch" href="#fontai.runners.stages.Ingestion.transform_batch">transform_batch</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.runners.stages.Preprocessing" href="#fontai.runners.stages.Preprocessing">Preprocessing</a></code></h4>
<ul class="">
<li><code><a title="fontai.runners.stages.Preprocessing.get_stage_name" href="#fontai.runners.stages.Preprocessing.get_stage_name">get_stage_name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fontai.runners.stages.Scoring" href="#fontai.runners.stages.Scoring">Scoring</a></code></h4>
<ul class="">
<li><code><a title="fontai.runners.stages.Scoring.CHARSET_OPTIONS" href="#fontai.runners.stages.Scoring.CHARSET_OPTIONS">CHARSET_OPTIONS</a></code></li>
<li><code><a title="fontai.runners.stages.Scoring.fit" href="#fontai.runners.stages.Scoring.fit">fit</a></code></li>
<li><code><a title="fontai.runners.stages.Scoring.from_config_object" href="#fontai.runners.stages.Scoring.from_config_object">from_config_object</a></code></li>
<li><code><a title="fontai.runners.stages.Scoring.get_stage_name" href="#fontai.runners.stages.Scoring.get_stage_name">get_stage_name</a></code></li>
<li><code><a title="fontai.runners.stages.Scoring.transform" href="#fontai.runners.stages.Scoring.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>